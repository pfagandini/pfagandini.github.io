\documentclass[aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{opensans}

\title{Matrices}
\author{Paulo Fagandini}
\institute{\includegraphics[scale=0.3]{NovaPrincipalV1.png}}
\date{}

\makeatletter
\setbeamertemplate{theorem begin}{%
  \setbeamercolor{block title}{bg=white,fg=black}%
  \setbeamercolor{itemize item}{fg=black}%
  \setbeamercolor{itemize subitem}{fg=black}%
  \setbeamercolor{itemize subsubitem}{fg=black}%
  \setbeamercolor{enumerate item}{fg=black}%
  \setbeamercolor{enumerate subitem}{fg=black}%
  \setbeamercolor{enumerate subsubitem}{fg=black}%
  \begin{\inserttheoremblockenv}
    {%
      \textbf{\inserttheoremname}
      %\inserttheoremnumber
      \ifx\inserttheoremaddition\@empty\else\ \inserttheoremaddition\fi%
    }%
    \normalfont%
}

\setbeamertemplate{theorem end}{%
    \end{\inserttheoremblockenv}%
}

\setbeamercolor{block title}{bg=white,fg=black}%
\setbeamercolor{itemize item}{fg=black}%
\setbeamercolor{itemize subitem}{fg=black}%
\setbeamercolor{itemize subsubitem}{fg=black}%
\setbeamercolor{enumerate item}{fg=black}%
\setbeamercolor{enumerate subitem}{fg=black}%
\setbeamercolor{enumerate subsubitem}{fg=black}%

\makeatother

\newtheorem{defenition}{Definition}[section]
\newtheorem{proposition}{Conjecture}[section]

\AtBeginSection{\frame{\sectionpage}}

\setbeamertemplate{footline}{
    \hspace{0.05\textwidth}
    \raisebox{3ex}{\insertshortauthor{}}\hfill
    \raisebox{3ex}{\insertframenumber{}/\inserttotalframenumber} \hfill
    {\raisebox{3ex}{\includegraphics[height=0.04\textheight]{NovaPrincipalV2.png}}}
    \hspace{0.05\textwidth}
}

\setbeamertemplate{navigation symbols}{}

\setbeamercolor{title}{fg = black}
\setbeamercolor{subtitle}{fg = black}
\setbeamercolor{frametitle}{fg = white, bg = black}
\setbeamercolor{title example}{fg=black} 

\hypersetup{linkcolor=black, colorlinks=true}

\begin{document}

{
\setbeamertemplate{footline}{} 
\begin{frame}
  \titlepage
\end{frame}
}

\begin{frame}{Introduction}
    \begin{center}
        \begin{tikzpicture}[scale=0.6,every node/.style = {scale = 0.6}]
            \draw[<->,thick] (-5,0) -- (5,0);
            \draw[<->,thick] (0,-3) -- (0,5);
            
            \onslide<2->{
                \draw[->,thick,orange] (0,0) -- (-2,1)node[left]{$v$};
            }
        
            \onslide<3->{
                \draw[->,red] (0,0) -- (1,0)node[below]{\(\hat{\imath}\)};
                \draw[->,blue] (0,0) -- (0,1)node[right]{\(\hat{\jmath}\)};
            }
            
        \end{tikzpicture}
        \onslide<2>{\[v=\begin{bmatrix}-2\\1\end{bmatrix}\]}
        \onslide<3>{\[v=-2\times\hat{\imath}+1\times\hat{\jmath}=-2\times\begin{bmatrix}1\\0\end{bmatrix}+1\times\begin{bmatrix}0\\1\end{bmatrix}=\begin{bmatrix}-2\\1\end{bmatrix}\]}
    \end{center}
\end{frame}

\begin{frame}{Introduction}
    Being not very rigorous, we can define a linear transformation as a transformation on every vector on the plane that must satisfy 2 things:
    \begin{enumerate}
        \item Lines must be transformed into lines
        \item The origin must remain in the same place
    \end{enumerate}
    We will deal with the formal definition and rigor later...
    
\end{frame}

\begin{frame}{Introduction}
    \begin{center}
        \begin{tikzpicture}[scale=0.6,every node/.style = {scale = 0.6}]
            \draw[<->,thick,opacity=0.3] (-6,0) -- (5,0);
            \draw[<->,thick,opacity=0.3] (0,-3) -- (0,5);
            \draw[->,thick,orange,opacity=0.3] (0,0) -- (-2,1)node[left]{$v$};
            
            \draw[->,red,opacity=0.4] (0,0) -- (1,0)node[below]{\(\hat{\imath}\)};
            \draw[->,blue,opacity=0.4] (0,0) -- (0,1)node[right]{\(\hat{\jmath}\)};
            
            \draw[<->,thick] (-6,-1.5) -- (6,1.5);
            \draw[<->,thick] (-4.5,-3) -- (4.5,3);

            \onslide<1->{
                \draw[->,red] (0,0) -- (4,1)node[below]{\(\hat{\imath}'\)};
                \draw[->,blue] (0,0) -- (3,2)node[below]{\(\hat{\jmath}'\)};
            }
            
            \onslide<4->{
                \draw[->,thick,orange] (0,0) -- (-5,0)node[below]{$v'$};
            }
            
        \end{tikzpicture}
        \only<2>{\[\hat{\imath}'=\begin{bmatrix}4\\1\end{bmatrix}\quad\hat{\jmath}'=\begin{bmatrix}3\\2\end{bmatrix}\]}
        \only<3>{\[v=-2\times\hat{\imath}'+1\times\hat{\jmath}'=-2\times\begin{bmatrix}4\\1\end{bmatrix}+1\times\begin{bmatrix}3\\2\end{bmatrix}=\begin{bmatrix}-5\\0\end{bmatrix}\]}
        \only<5>{\[w=\begin{bmatrix}x\\y\end{bmatrix} \text{ lands on } x\times \begin{bmatrix}4\\1\end{bmatrix}+y\times \begin{bmatrix}3\\2\end{bmatrix}=\begin{bmatrix}4x+3y\\1x+2y\end{bmatrix}\]}
        \only<6>{\[\begin{bmatrix}1&0\\0&1\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}x\\y\end{bmatrix}=x\times \hat{\imath}+y\times \hat{\jmath}\]
        \[\begin{bmatrix}4&3\\1&2\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}4x+3y\\1x+2y\end{bmatrix}=x\times\hat{\imath}'+y\times\hat{\jmath}'\]}
    \end{center}
\end{frame}

\begin{frame}{Introduction}
    What about another vector in the same ``direction'' than $v$? say \[z=\begin{bmatrix}1\\-0.5\end{bmatrix}\] ... \pause
    
    \[\begin{bmatrix}4&3\\1&2\end{bmatrix}\begin{bmatrix}1\\-0.5\end{bmatrix}=\begin{bmatrix}2.5\\0\end{bmatrix}\]
\end{frame}

\begin{frame}{Introduction}
    \begin{center}
        \begin{tikzpicture}[scale=0.6,every node/.style = {scale = 0.6}]
            \draw[<->,thick,opacity=0.3] (-6,0) -- (5,0);
            \draw[<->,thick,opacity=0.3] (0,-3) -- (0,5);
            \draw[->,thick,orange,opacity=0.3] (0,0) -- (-2,1)node[left]{$v$};
            \draw[->,thick,orange,opacity=0.3] (0,0) -- (1,-0.5)node[left]{$z$};
            
            \draw[->,red,opacity=0.4] (0,0) -- (1,0)node[below]{\(\hat{\imath}\)};
            \draw[->,blue,opacity=0.4] (0,0) -- (0,1)node[right]{\(\hat{\jmath}\)};
            
            \draw[<->,thick] (-6,-1.5) -- (6,1.5);
            \draw[<->,thick] (-4.5,-3) -- (4.5,3);

            \onslide<1->{
                \draw[->,red] (0,0) -- (4,1)node[below]{\(\hat{\imath}'\)};
                \draw[->,blue] (0,0) -- (3,2)node[below]{\(\hat{\jmath}'\)};
            }
            
            \onslide<1->{
                \draw[->,thick,orange] (0,0) -- (2.5,0)node[below]{$z'$};
                \draw[->,thick,orange] (0,0) -- (-5,0)node[below]{$v'$};
            }
            
        \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}{Introduction}
    So transforming two vectors in the same line, they both end up also in the same line... keep this in mind.
\end{frame}

\begin{frame}{Introduction}
    Could we take back $\hat{\imath}'$ to $\hat{\imath}$ and $\hat{\jmath}'$ to $\hat{\jmath}$? \pause
    
    Sure, if we apply the transform \[\begin{bmatrix}0.4&-0.6\\-0.2&0.8\end{bmatrix}\] \pause
    
    \[\begin{bmatrix}0.4&-0.6\\-0.2&0.8\end{bmatrix}\begin{bmatrix}4&3\\1&2\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}\]
    
    Applying the inverse!
\end{frame}

\begin{frame}{Introduction}
    The important thing, is that if the vectors are linearly dependent, then we cannot invert the matrix, we just saw that two vectors that reside on the same line, end up in the same (although probably a different one) line.
\end{frame}

\begin{frame}{Introduction}
    There are a couple of interesting vectors on the whole space when we apply this linear transformation...
    
    Take for example the following vector:\(e=\begin{bmatrix}-1\\1\end{bmatrix}\)\pause
    \[\begin{bmatrix}4&3\\1&2\end{bmatrix}\begin{bmatrix}-1\\1\end{bmatrix}=\begin{bmatrix}-1\\1\end{bmatrix}\]
\end{frame}

\begin{frame}{Introduction}
    \begin{center}
        \begin{tikzpicture}[scale=0.6,every node/.style = {scale = 0.6}]
            \draw[<->,thick,opacity=0.3] (-6,0) -- (5,0);
            \draw[<->,thick,opacity=0.3] (0,-3) -- (0,5);

            \draw[->,red,opacity=0.4] (0,0) -- (1,0)node[below]{\(\hat{\imath}\)};
            \draw[->,blue,opacity=0.4] (0,0) -- (0,1)node[right]{\(\hat{\jmath}\)};
            
            \draw[<->,thick] (-6,-1.5) -- (6,1.5);
            \draw[<->,thick] (-4.5,-3) -- (4.5,3);

            \onslide<1->{
                \draw[->,red] (0,0) -- (4,1)node[below]{\(\hat{\imath}'\)};
                \draw[->,blue] (0,0) -- (3,2)node[below]{\(\hat{\jmath}'\)};
            }
            \onslide<1>{
                \draw[->,orange,thick](0,0)--(-1,1)node[left]{\(e\)};
            }
            
            \onslide<2>{
                \draw[->,orange,thick](0,0)--(-1,1)node[left]{\(e=e'\)};
            }
            
        \end{tikzpicture}
        \only<2>{
            \[
            \begin{bmatrix}
                1&0\\0&1
            \end{bmatrix}
            \begin{bmatrix}
                -1\\1
            \end{bmatrix}=\begin{bmatrix}
                -1\\1
            \end{bmatrix}=
            \begin{bmatrix}
                4&3\\1&2
            \end{bmatrix}
            \begin{bmatrix}
                -1\\1
            \end{bmatrix}\]
            }
            \only<3>{\[e.v._1=\begin{bmatrix}-1,1\end{bmatrix},\quad \lambda_1=1\]}
    \end{center}
\end{frame}

\begin{frame}{Introduction}

    Or the vector:\(f=\begin{bmatrix}0.9487\\0.3162\end{bmatrix}\)\pause
    \[\begin{bmatrix}4&3\\1&2\end{bmatrix}\begin{bmatrix}0.9487\\0.3162\end{bmatrix}=\begin{bmatrix}4.7434\\1.5811\end{bmatrix}\]
    
\end{frame}

\begin{frame}{Introduction}
    \begin{center}
        \begin{tikzpicture}[scale=0.6,every node/.style = {scale = 0.6}]
            \draw[<->,thick,opacity=0.3] (-6,0) -- (5,0);
            \draw[<->,thick,opacity=0.3] (0,-3) -- (0,5);

            \draw[->,red,opacity=0.4] (0,0) -- (1,0)node[below]{\(\hat{\imath}\)};
            \draw[->,blue,opacity=0.4] (0,0) -- (0,1)node[right]{\(\hat{\jmath}\)};
            
            \draw[<->,thick,opacity=0.6] (-6,-1.5) -- (6,1.5);
            \draw[<->,thick,opacity=0.6] (-4.5,-3) -- (4.5,3);

            \onslide<1->{
                \draw[->,red] (0,0) -- (4,1)node[below]{\(\hat{\imath}'\)};
                \draw[->,blue] (0,0) -- (3,2)node[below]{\(\hat{\jmath}'\)};
                \draw[->,orange,thick](0,0)--(-1,1)node[below]{e=e'};
                \draw[->,orange,thick](0,0)--(0.9487,0.3162)node[below]{f};
            }
            
            \onslide<2>{
                \draw[->,orange,thick](0,0)--(4.7434,1.5811)node[below]{f'};
            }
            
        \end{tikzpicture}
        \only<2>{
            \[
            5\times
            \begin{bmatrix}
                1&0\\0&1
            \end{bmatrix}
            \begin{bmatrix}
                0.9487\\0.3162
            \end{bmatrix}=\begin{bmatrix}
                4.7434\\1.5811
            \end{bmatrix}=
            \begin{bmatrix}
                4&3\\1&2
            \end{bmatrix}
            \begin{bmatrix}
                0.9487\\0.3162
            \end{bmatrix}\]
            }
            \only<3>{\[e.v._2=\begin{bmatrix}0.9487\\0.3162\end{bmatrix},\quad \lambda_2=5\]}
    \end{center}
\end{frame}

\begin{frame}{Introduction}
    What happens with these vectors?
    
    \begin{table}[]
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{ccc}
            & \(ev_1\) & \(ev_2\)\\\hline
            \(A\times\) & \(\begin{bmatrix}-1\\1\end{bmatrix}\) & \(\begin{bmatrix}4.7434\\1.5811\end{bmatrix}\) \\
            \onslide<2->{\(A^2\times\) & \(\begin{bmatrix}-1\\1\end{bmatrix}\) & \(\begin{bmatrix}23.7171\\7.9057\end{bmatrix}\)  }\\
            \onslide<3->{\(A^3\times\) & \(\begin{bmatrix}-1\\1\end{bmatrix}\) & \(\begin{bmatrix}118.585\\39.528\end{bmatrix}\)} \\\hline
            \onslide<4->{\(\lambda\) & 1 & 5}
        \end{tabular}
    \end{table}
    
\end{frame}

\begin{frame}
    \begin{definition}
        A real \textbf{matrix} is a rectangular array of real numbers.
        
        \begin{align*}
            A = \left(
            \begin{array}{cccc}
                a_{11} & a_{12} & \ldots & a_{1n}\\
                a_{21} & a_{22} & \ldots & a_{2n}\\
                \vdots & \vdots & \ddots & \vdots \\
                a_{m1} & a_{m2} & \ldots & a_{nm} 
            \end{array}
            \right)
        \end{align*}
        
        Where $a_{ij}\in\mathbb{R}$. $A$ is said to be an element of $\mathbb{R}^{m\times n}$
    \end{definition}
    
    A vector would be then a matrix with only 1 column!
\end{frame}

\begin{frame}
    Let $A,B\in\mathbb{R}^{m\times n}$. Let $C\in\mathbb{R}^{n\times l}$. Finally, let $\alpha\in\mathbb{R}$.
    \begin{enumerate}
        \item $[A+B]_{ij} = a_{ij}+b_{ij}$
        \item $[A \cdot C]_{ik}=\sum_{j=1}^n a_{ij}\cdot c_{jk}$, and it has a dimension $m\times l$
        \item $[\alpha A]_{ij}=\alpha a_{ij}$
    \end{enumerate}
\end{frame}

\begin{frame}
    \begin{definition}
        Let $A\in\mathbb{R}^{m\times n}$, $A$'s \textbf{transpose}, denoted $A^t\in\mathbb{R}^{n\times m}$ is such that its elements are:
        
        \begin{align*}
            a^t_{ij}=a_{ji}
        \end{align*}
    \end{definition}
    
    \begin{definition}
        Matrix $A\in\mathbb{R}^{m\times n}$ is said to be \textbf{squared} if $n=m$
    \end{definition}
    
    \begin{definition}
        Matrix $A$ is said to be \textbf{symmetric} if $A^t=A$
    \end{definition}
    \begin{definition}
        Matrix $A$ is said to be \textbf{antisymmetric} if $A^t=-A$
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        The \textbf{Identity} is a squared matrix $I_n\in\mathbb{R}^{n\times n}$ that has $I_{ij}=0$ if $i\neq j$, and $I_{ij}=1$ if $i=j$.
    \end{definition}
    
    The identity has a nice property: $A I_n = I_m A= A$ for any $A\in\mathbb{R}^{m\times n}$.
    
    \begin{definition}
        Matrix $A$ is \textbf{invertible}, if there is another matrix $A^{-1}$ such that $A\cdot A^{-1} = A^{-1}\cdot A = I$
    \end{definition}
\end{frame}

\begin{frame}
    \begin{proposition}
        Given $A,B,C\in\mathbb{R}^{n\times n}$
        
        \begin{enumerate}
            \item $A+B=B+A$
            \item $A(BC)=(AB)C$
            \item $A(B+C)=AB+AC$
            \item $(A+B)^t = A^t+B^t$
            \item $(AB)^t=B^tA^t$
            \item $(A^t)^t=A$
            \item If $A$ and $B$ are invertible, then $AB$ and $BA$ are invertible as well. Furthermore $(AB)^{-1}=B^{-1}A^{-1}$
            \item If $A$ is invertible, then $(A^t)^{-1}=(A^{-1})^t$
        \end{enumerate}
    \end{proposition}
\end{frame}

\begin{frame}
    Quick quiz, 15 min, prove points 7 and 8. You can use points 1-6 as true and given.
\end{frame}

\begin{frame}{Solution}
    \begin{enumerate}
    
    \item[7] Start with $AB$, multiply by $A^{-1}$ from the left, you are left with $A^{-1}AB = Id B = B$. Now multiply by $B^{-1}$, so you get $B^{-1}A^{-1}AB = B^{-1}IdB=B^{-1}B=Id$. Then $(B^{-1}A^{-1})(AB) = Id$ so it must be that $B^{-1}A^{-1}=(AB)^{-1}$. To complete the proof, you need to show that you can do the same from the ``right''.
    
    \item[8] Start with $(A^{-1}A)^t = Id ^t = Id$, use property 5 and you get $(A^{-1}A)^t = A^t (A^{-1})^t = Id$, then $(A^{-1})^t$ must be the inverse (again the only thing that is missing is to show that it works if you start with $(A A^{-1})^t$ as well which is trivial.
    
    \end{enumerate}
\end{frame}

\begin{frame}
\begin{proposition}
    The set of the matrices in $\mathbb{R}^{m\times n}$, together with the sum and scalar multiplication is a vector space.    
\end{proposition}

\begin{proposition}
A squared matrix $A\in\mathbb{R}^{n\times n}$ is invertible if and only if all of its columns are linearly independent.
\end{proposition}
\end{frame}


\begin{frame}
    \begin{definition}
        Matrix $A\in\mathbb{R}^{m\times n}$ is \textbf{upper triangular} if it has the following shape:
        
        \begin{align*}
            A=\left[
            \begin{array}{cccccc}
                a_{11} & a_{12} & \ldots & a_{1m} & \ldots & a_{1n}\\
                0 & a_{22} & \ldots & a_{2m} & \ldots & a_{2n}\\
                0 & 0 & \ddots & \vdots & \ldots& \vdots\\
                0 & 0 & \ldots & a_{mm} & \ldots & a_{mn}
            \end{array}
            \right]
        \end{align*}
        
        That is, it has zeroes below its main diagonal.
    \end{definition}
    
    \begin{proposition}
        The set of the upper triangular matrices in $\mathbb{R}^{n\times m}$, with the sum and scalar multiplication is a vector subspace of $\mathbb{R}^{n\times m}$.
    \end{proposition}
\end{frame}

\begin{frame}
    \begin{definition}
        Matrix $A$ is \textbf{lower triangular} if $A^t$ is upper triangular.
    \end{definition}

    \begin{definition}
        Matrix $A$ is \textbf{diagonal} if it is upper and lower triangular at the same time.
    \end{definition}

    \begin{definition}
        The \textbf{rank} of a matrix $A$, denoted by $rank(A)$ is the maximum number of linearly independent rows or columns of $A$.
    \end{definition}
\end{frame}

\begin{frame}
    A convenient way to write down a system of equations:
    \begin{align*}
        \begin{array}{ccccccccc}
        a_{11} x_1 &+& a_{12} x_2 &+&\ldots &+& a_{1n}x_n  &=& b_1\\
        a_{21} x_1 &+& a_{22} x_2 &+&\ldots &+& a_{1n}x_n  &=& b_2\\
        \vdots & &  \vdots & &\ldots & & \vdots  & & \vdots\\
        a_{m1} x_1 &+& a_{m2} x_2 &+&\ldots &+& a_{mn}x_n  &=& b_m
        \end{array}
    \end{align*}
    
    It would be $A X = B$, where,
    \begin{align*}
        A=\left(\begin{array}{ccc}a_{11}&\ldots&a_{1n}\\ \vdots & \ddots & \vdots \\ a_{m1}&\ldots&a_{mn}\end{array}\right)\quad X=\left(\begin{array}{c}x_1\\ \vdots \\ x_n\end{array}\right) \quad
        B = \left(\begin{array}{c}
             b_1\\\vdots \\ b_m
        \end{array}\right)
    \end{align*}
\end{frame}

\begin{frame}
\begin{definition}
    Given a system of equations $AX=B$, \begin{itemize}
        \item $\hat{X}\in\mathbb{R}^n$ is a \textbf{particular solution} of the system if $A\hat{X}=B$.
        \item $X_0$ is an \textbf{homogeneous solution} if $AX_0=0$.
    \end{itemize}
    
    
\end{definition}
    
    Note that for $\lambda\in\mathbb{R}$, $A(\hat{X}+\lambda X_0)=A\hat{X}+\lambda A X_0 = B+0 = B$.
    
    \vspace{0.5cm}
    
\end{frame}

\begin{frame}
    \begin{definition}
        The \textbf{kernel} of $A\in\mathbb{R}^{m\times n}$ is defined as:
        \begin{align*}
            Ker(A):=\{X\in\mathbb{R}^n| AX = 0\}
        \end{align*}
    \end{definition}
    
    \begin{proposition}
        $Ker(A)\subseteq\mathbb{R}^n$ is a vector subspace of $\mathbb{R}^n$
    \end{proposition}
    
    \begin{definition}
        The dimension of $Ker(A)$ is called the \textbf{nullity} --- or nullspace ---, and it is denoted by $Null(A)$. If $Ker(A)=\{0\}$, then $Null(A)=0$.
    \end{definition}
    
    Note that a system of equations as the one shown before, has unique solution only if $Null(A)=0$.
    
\end{frame}

\begin{frame}
    \begin{proposition}
        Matrix $A\in\mathbb{R}^{n\times n}$ is invertible if and only if $Null(A)=0$.
    \end{proposition}
    
    \begin{proposition}
        $A\in\mathbb{R}^{n\times n}$ is invertible if and only if the system $AX=B$ as a unique solution, for any $B\in\mathbb{R}^n$.
    \end{proposition}
    
\end{frame}

\begin{frame}
    \begin{definition}
        The \textbf{image} of $A\in\mathbb{R}^{n\times n}$ is defined as:
        \begin{align*}
            Im(A):=\{Y\in\mathbb{R}^n|\exists X\in\mathbb{R}^n, Y=AX\}\equiv \{AX|X\in\mathbb{R}^n\}
        \end{align*}
    \end{definition}
    
    \begin{proposition}
        Let $A\in\mathbb{R}^{n\times n}$. $Im(A)$ is a vector subspace of $\mathbb{R}^n$.
    \end{proposition}
    
    \begin{definition}
        The dimension of $Im(A)$ is called the \textbf{range} of $A$. Let's denote it as $R(A)$.
    \end{definition}
\end{frame}

\begin{frame}
    \begin{proposition}
        Let $A\in\mathbb{R}^{n\times n}$. $R(A)$ is the number of l.i. columns of $A$.
    \end{proposition}
    
    \begin{proposition}
        Consider $A\in\mathbb{R}^{n\times n}$. It holds that $Null(A)+R(A)=n$.
    \end{proposition}
\end{frame}
\iffalse
\begin{frame}
    \begin{proposition}
        For any $A\in\mathbb{R}^{n\times n}$, there is an invertible matrix $C$, and an upper triangular matrix $R$ such that, $$CA=R\Leftrightarrow A=C^{-1}R$$
    \end{proposition}
\end{frame}
\fi
\begin{frame}
    \begin{definition}
        The quadratic form associated to $A$ is a function $Q_A:\mathbb{R}^n\rightarrow\mathbb{R}$ such that for any $X\in\mathbb{R}^n$,
        
        $$Q_A(X)=X^tAX\in\mathbb{R}$$
    \end{definition}
\end{frame}

\begin{frame}

\begin{proposition}
    For any matrix $A\in\mathbb{R}^n$, there are always symmetric and antisymmetric matrices $S$ and $T$ such that $$A=S+T$$
\end{proposition}

Note:  Let $S=\frac{A+A^t}{2}$ and $T=\frac{A-A^t}{2}$. While $S$ is symmetric, $T$ is antisymmetric,
    
    
    \begin{corollary}
        A quadratic form can be represented as $$Q_A(X)=X^t S X$$ with $S$ symmetric.
    \end{corollary}
\end{frame}

\begin{frame}
    Quick quiz! 15 min to prove the corollary.
\end{frame}

\begin{frame}{Solution}
    
    \begin{itemize}
        \item Let $A = (S+T)$
        \item Then $Q_A(X) = X^t(S+T)X = X^tSX + X^tTX$
        \item But $X^tTX\in \mathbb{R}$, so $(X^tTX)^t=X^tTX$ (a number trasposed is the same number).
        \item So you end up that $X^tTX = (X^tTX)^t = X^tT^tX$
        \item But $T$ is anytsymmetric so $T^t = -T$...
        \item Then $X^tTX = -X^tTX$, so if $X^tTX$ is the number $z$, you have $z=-z$, that only is true for $z=0$.
        \item Then $Q_A(X) = X^tSX$
    \end{itemize}
    
\end{frame}

\begin{frame}
    \begin{definition}
        Let $A\in\mathbb{R}^{n\times n}$, symmetric. Consider the quadratic form $Q_A(X)=X^t A X$. If for any $X\in\mathbb{R}^n\setminus\{0\}$,
        \begin{enumerate}
            \item $Q_A(X)>0$, $A$ is \textbf{positive definite}, \item $Q_A(X)\geq0$, $A$ is \textbf{positive semi-definite}, \item $Q_A(X)<0$, $A$ is \textbf{negative definite}, \item $Q_A(X)\leq0$, $A$ is \textbf{negative semi-definite}.
        \end{enumerate}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        $\lambda\in\mathbb{C}$ is an \textbf{eigenvalue} (or characteristic value) of matrix $A\in\mathbb{R}^{n\times n}$ if there is a vector, called \textbf{eigenvector}, $X_\lambda\in\mathbb{R}^n\setminus\{0\}$ such that $$A X_\lambda = \lambda X_\lambda$$
    \end{definition}
    
\end{frame}

\begin{frame}
    \begin{proposition}
        Let $A\in\mathbb{R}^{n\times n}$, and $\lambda_1$ and $\lambda_2$ two eigenvalues of $A$, with $\lambda_1\neq\lambda_2$. If $V_1$ in the vector subspace associated to $\lambda_1$, and $V_2$ in the vector subspace of $\lambda_2$ then $V_1$ and $V_2$ are linearly independent.
    \end{proposition}
    
    \begin{proposition}
        Given $A\in\mathbb{R}^{n\times n}$ symmetric, then its eigenvalues are real valued.
    \end{proposition}
\end{frame}

\begin{frame}
    \begin{definition}
        The \textbf{determinant} of a squared matrix $A$ is the hyper-volume of the figure formed by the column vectors of the matrix.
    \end{definition}
    
    \begin{example}
        Consider the matrices,
        
        \begin{align*}
            A=\left(
            \begin{array}{cc}
                1/2 & 2 \\
                3/2 & 1
            \end{array}
            \right) \quad
            B=\left(
            \begin{array}{cc}
                1 & 2\\
                -1 & -2
            \end{array}
            \right)
        \end{align*}
    \end{example}
    
\end{frame}

\begin{frame}

    \begin{center}
        \begin{tikzpicture}
            \draw[->](-0.5,0)--(4,0);
            \draw[->](0,-3)--(0,3.5);
            
            \draw[red,->](0,0)--(0.5,1.5);
            \draw[red,->](0,0)--(2,1);
            
            \draw[red,dashed](0.5,1.5)--(2.5,2.5);
            \draw[red,dashed](2,1)--(2.5,2.5);
            
            \draw[blue,->](0,0)--(1,-1);
            \draw[blue,->](0,0)--(2,-2);
            
        \end{tikzpicture}
    \end{center}
    
    It is easy to see that, given our definition, $det(B)=0$. It is also easy to show that $det(A)=|1/2\times1-3/2\times2|=5/2$.
    
\end{frame}

\begin{frame}
    How to calculate the determinant of a big matrix? Recursively. Let $A\in\mathbb{R}^{n\times n}$.
    Define $A_{ij}$ as:
    
    \begin{align*}
        A_{ij}=\left(
        \begin{array}{cccccc}
             a_{11} & \ldots & a_{1,j-1} & a_{1,j+1} & \ldots & a_{1n}  \\
             \vdots& \vdots & \vdots & \vdots & \vdots  & \vdots \\
             a_{i-1,1} & \ldots & a_{i-1,j-1} & a_{i-1,j+1} & \ldots & a_{i-1,n}\\
             a_{i+1,1} & \ldots & a_{i+1,j-1} & a_{i+1,j+1} & \ldots & a_{i+1,n}\\
             \vdots& \vdots & \vdots & \vdots & \vdots  & \vdots \\
             a_{n1} & \ldots & a_{n,j-1} & a_{n,j+1} & \ldots & a_{nn}  
        \end{array}
        \right)
    \end{align*}
    
    That is, what is left of $A$ after removing row $i$ and column $j$.
\end{frame}

\begin{frame}

    Then,
    \begin{align*}
        det(A)=\sum_{k=1}^n (-1)^{i+k} a_{ik} det(A_{ik})
    \end{align*}
    
    You can choose any $i$ that you prefer.
    
\end{frame}

\begin{frame}
    \begin{proposition}
    
    \begin{itemize}
        \item A squared matrix is invertible if and only if its determinant is different from zero.
        \item Take a finite set of matrices $\mathbb{A}\subseteq\mathbb{R}^{n\times n}$, with $A_i$ being the $ith$ element of $\mathbb{A}$ then, $$det(A_1A_2\ldots A_k)=det(A_1)det(A_2)\ldots det(A_k)$$
        \item If $A$ is invertible, then $$det(A^{-1})=\frac{1}{det(A)}$$
        \item For any squared $A$ it holds that $det(A^t)=det(A)$.
    \end{itemize}
        
    \end{proposition}
    
    
    
\end{frame}

\begin{frame}
    Note that $\lambda$ is an eigenvalue if $$A X_{\lambda}=\lambda X_\lambda\quad\quad\text{with}\quad X_\lambda\neq0$$ so $\lambda$ is an eigenvalue of $A$ if $$(A-\lambda I)X_\lambda =0$$ or $X_\lambda\in ker(A-\lambda I)$, which implies that $ker(A-\lambda I)\neq \{0\}$, and therefore $(A-\lambda I)$ must be not invertible! But if $(A-\lambda I)$ is not invertible, then $det(A-\lambda I) = 0$. 
    
    \begin{corollary}
        $\lambda$ is an eigenvalue of $A$ if and only if $det(A-\lambda I)=0$.
    \end{corollary}
\end{frame}

\begin{frame}
    \begin{definition}
    Given $A\in\mathbb{R}^{n\times n}$, the \textbf{characteristic polynomial} of $A$ is defined as the function $p_A:\mathbb{R}\rightarrow\mathbb{R}$ such that $$p_A(\lambda)=det[A-\lambda I]$$
    \end{definition}

    So, $\lambda$ is an eigenvalue of $A$ if $p_A(\lambda)=0$

\end{frame}

\begin{frame}
    \begin{proposition}
        If $A$ is symmetric, then the eigenvectors of different eigenvalues are orthogonal.
    \end{proposition}
    
    For practical reasons, consider the matrix $V$ as the matrix that has in its columns the eigenvectors of $A$, and $D(\lambda)$ the diagonal matrix that contains in the column $i$, the eigenvalue that corresponds to the eigenvector in the column $i$ in $V$.
    
    Note that:
    $$ A V = V D(\lambda)\quad\Leftrightarrow\quad A = V D(\lambda) V^{-1}$$
    
    It can be shown that, given that the eigen vectors are l.i., $V^t=V^{-1}$ and therefore $$A=V D(\lambda) V^{-1}$$ which is one of the fundamental properties of the symmetric matrices.
\end{frame}

\begin{frame}
    \begin{proposition}
        Given $A\in\mathbb{R}^{n\times n}$, symmetric. It holds that,
        \begin{align*}
            A  = V D V^t
        \end{align*}
        
        With $D$ the diagonal with the eigenvalues of $A$ and $V$ the unit eigenvectors of $A$.
    \end{proposition}
\end{frame}

\begin{frame}
    \begin{proposition}
        Let $A\in\mathbb{R}^{n\times n}$, symmetric.
        
        \begin{enumerate}
            \item $A$ is positive definite if all the eigenvalues of $A$ are strictly positive.
            \item $A$ is positive semi definite if all the eigenvalues are nonnegative.
            \item $A$ is negative definite if all the eigenvalues are strictly negative.
            \item $A$ is negative semidefinite if all the eigenvalues are non positive.
        \end{enumerate}
    \end{proposition}
\end{frame}

\begin{frame}
\begin{definition}
    A function $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is linear if, for any $X,Y\in\mathbb{R}^n$, and for any $\alpha\in\mathbb{R}$
    \begin{align*}
        f(X+Y)=f(X)+f(Y),\quad f(\alpha X)=\alpha f(X)
    \end{align*}
\end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        The \textbf{trace} of a square matrix $A$ ($tr(A)$) is the sum of the elements of its diagonal.
    \end{definition}
\end{frame}

\begin{frame}
    \begin{theorem}
    Let $A\in\mathbb{R}^{n\times n}$, then:
    \begin{itemize}
        \item the product of the eigenvalues of $A$ is equal to its determinant, that is,\[det(A)=\prod_{i=1}^n \lambda_i\]
        \item the sum of the eigenvalues of $A$ is equal to its trace, that is, \[\sum_{i=1}^n a_{i,i} =\sum_{i=1}^n \lambda_i\]
        \item if $A$ is a triangular matrix, then its eigenvalues are the coefficients in the principal diagonal of the matrix, \emph{i.e.}, \[\lambda_i = a_{i,i}\]
    \end{itemize}
    \end{theorem}
\end{frame}

\end{document}
