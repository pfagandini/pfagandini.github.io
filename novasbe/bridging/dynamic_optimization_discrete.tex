\documentclass[aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage{opensans}

\usetheme{NOVASBE}

\title[]{4509 - Bridging Mathematics}
\subtitle{Dynamic Optimization:\\Euler and how to\\ optimally eat cake}
\author[P. Fagandini]{Paulo Fagandini}
\institute{}
\date{}

\newtheorem{defenition}{Definition}[section]
\newtheorem{proposition}{Conjecture}[section]

\begin{document}

\begin{frame}{Dynamic Optimization: Basic Problem}
    
    In this section we review the basics/intuition of dynamic optimization. We are going to solve how to properly eat a cake!
    
    \begin{figure}
        \centering
        \includegraphics[scale = 0.8]{cake.png}
        \caption{How would you eat this cake... if it was the only food you would ever get!}
        \label{fig:cake}
    \end{figure}
\end{frame}

\begin{frame}{Cake Eating Problem}
    
    \begin{enumerate}
        \item Utility function $u(c) = \ln(c)$, where $c$ is the slice of the cake you are eating.
        \item You have a cake of size $x$.
        \item Your discount factor is $\beta\in[0,1)$.
        \item You live forever.
    \end{enumerate}
    
\end{frame}

\begin{frame}{Cake Eating Problem}
    
    So at each point in time:
    \begin{enumerate}
        \item you have $x_t$ of cake
        \item you get $\ln(c_t)$ of utility, and
        \item you leave $x_{t+1} = x_t - c_t$ for the future.
    \end{enumerate}
    
    Your only decision variable is \emph{how much} cake eat at each period, which impacts on your utility today, but also on how much utility you will be able to get in the future.
    
\end{frame}

\begin{frame}{Cake Eating Problem}
    
    So your problem is:
    
    \begin{align*}
        \sup_{\{c_t\}_t^\infty}\quad \sum_{t=0}^\infty \beta^t \ln(c_t)\\
        s.t. \quad x_{t+1} = x_t - c_t\\
        c_t \geq 0 \quad \forall t\\
        x_t \geq 0 \quad \forall t\\
        x_0 > 0 \quad given
    \end{align*}
    
    Keep this in mind...
    
\end{frame}

\begin{frame}{Gen. Seq. Opt. Problem}

Given $\beta\in[0,1)$

\begin{align*}
    \sup_{\{x_t\}_{t=1}^\infty} \sum_{t=0}^\infty \beta^t F(x_t,x_{t+1})\\
    s.t. \quad x_{t+1} \in \Gamma(x_t)\\
    x_0 \in X \subseteq \mathds{R}^n
\end{align*}
    
    With $\Gamma(x_t)\neq \emptyset$ and  $\Gamma(x_t)\subseteq X$, that is, only allow for feasible values for $x_t$. $X$ is known as the state space, and $x_t$ is then known as the... \pause you guessed it? \pause state variable.
    
\end{frame}

\begin{frame}

We can write what is the problem, using the notation we got from the set part...

\begin{align*}
    A& = \{(x,y) : x\in X, y \in \Gamma(x) \}\\
    F&: A \rightarrow \mathds{R}
\end{align*}

And we actually can choose from

\begin{align*}
    \Pi(x_0) = \{\{x_t\}_{t=0}^\infty , x_t \in \Gamma(x_{t-1}) , t \in \mathds{N} \}
\end{align*}

So $\Pi(x_o)$ represents the set of \emph{admissible paths} starting at $x_0$, and therefore the generic problem is equivalent to writing:
\begin{align*}
    \sup_{\{x_t\}_{t=1}^\infty\in\Pi(x_0)} \sum_{t=0}^\infty \beta^t F(x_t,x_{t+1})
\end{align*}

with $x_0$ given.

\end{frame}

\begin{frame}
    So how we solve this. It would help to get an idea of what we could expect of a solution.
    
    Of course we cannot find $x_t$ for every $t$ explicitly, as there are an infinite number of those, however, we can find a function to generate them, this function is called \emph{policy function}.
    \begin{align*}
        x_t = g(x_{t-1})
    \end{align*}
    
    Then the solution would look like... $$\{x_0, g(x_0), g(g(x_0)),...\}$$
    
\end{frame}

\begin{frame}{Cauchy's Criterion}

A real sequence $\{r_t\}$ converges in $\mathds{R}$ if and only if $\forall \epsilon > 0$ $\exists \mathds{T}$ such that $\forall t,s>\mathds{T}$ $|r_t - r_s| < \epsilon$
    
\end{frame}

\begin{frame}

So we want that for $\mathds{T}<T<S$,

\begin{align*}
    \left|\sum_{t=0}^T\beta^t F(x_t,x_{t+1}) - \sum_{t=0}^S \beta^t F(x_t,x_{t+1})\right| = \left|\sum_{t=T+1}^S\beta^tF(x_t,x_{t+1})\right|
\end{align*}

And 

\begin{align*}
    \left|\sum_{t=T+1}^S\beta^tF(x_t,x_{t+1})\right| \leq \sum_{t=T+1}^S\beta^t |F(x_t,x_{t+1})|
\end{align*}

\end{frame}

\begin{frame}{Assumption}

To find the solution we need an extra assumption, that $F(x_t,x_{t+1})$ is bounded!... $\exists M>0$ such that $\forall (x,y)\in A$ $|F(x,y)|\leq M$ 
    
\end{frame}


\begin{frame}

\begin{align*}
    \sum_{t=T+1}^S\beta^t |F(x_t,x_{t+1})| \leq \sum_{t=T+1}^S\beta^t M = M \sum_{t=T+1}^S\beta^t
\end{align*}

As $\beta\in[0,1)$
    
    \begin{align*}
        M \sum_{t=T+1}^S\beta^t \leq M \sum_{t=T+1}^\infty\beta^t = M\beta^{T+1} \sum_{t=0}^\infty\beta^t = M \beta^{T+1}\frac{1}{1-\beta}
    \end{align*}
    
    Now we want, from Cauchy, that
    
    \begin{align*}
        M \beta^{T+1}\frac{1}{1-\beta} < \epsilon
    \end{align*}
    
    Which can be achieved by choosing a sufficiently large $T$.
    
\end{frame}

\begin{frame}{What did just happen??}
    
    We saw that $\sum_{t=0}^\infty \beta^t F(x_t,x_{t+1})$ converges, so the objective function is well defined, if $F$ is bounded on the feasible domain, so $$\sum_{t=0}^\infty \beta^t F(x_t,x_{t+1}) \in \mathds{R}$$
    
    \vspace{0.5cm}
    
    The other assumption that would ensure a well defined objective function is $F\geq0$. Note that as $\beta\geq 0$ this would ensure that the sequence is strictly increasing in $T$, and therefore or it would reach a limit, or it could diverge to $+\infty$. What we cannot have is the sequence having more than one accumulation points, because we wouldn't know what happens at the end.
    
\end{frame}

\begin{frame}{Approaches}
    
    \begin{enumerate}
        \item Dynamic Programming
        \item Variational Approach: Euler's Equations
    \end{enumerate}
    
    \pause
    
    \vspace{0.5cm}
    
    We'll deal with Euler's equations here. Dynamic Programming although very useful is complex enough to be too much for a couple of hours lecture.
    
\end{frame}

\begin{frame}{Variational Approach}
    
    Say $x^*\in\mathds{R}^n$ is a maximizer of $f:\mathds{R}^n \rightarrow \mathds{R}$, then
    
        \begin{align*}
            f(x_1^*,x_2^*,x_3^*,...,x_n^*)\geq f(x_1,x_2,x_3,..., x_n)\quad\forall x\in\mathds{R}^n
        \end{align*}
    
    Which in turns implies that
    
    \begin{align*}
            f(x_1^*,x_2^*,x_3^*,...,x_n^*)\geq f(x_1,x_2^*,x_3^*,...,x_n^*)\quad\forall x_1\in\mathds{R}
        \end{align*}
    
    If $f$ is differentiable in $x_1$, then we would have $$f_{x_1}(x_1^*,x_2^*,x_3^*,...,x_n^*) = 0$$ as the first order condition. Moreover, we could generalize for each variable (assuming differentiability) to have
    
    $$f_{x_i}(x_i^*,x_{-i}^*)=0\quad \forall i = 1, ... , n$$
    
\end{frame}

\begin{frame}{Euler's Equations}
    Let $\beta\in(0,1)$ (note that if $\beta=0$ then the problem is not dynamic).
    
    \vspace{0.5cm}
    
    Let $\{x_t^*\}_{t=0}^\infty$ be such that $$\sum_{t=0}^\infty \beta^t F(x_t^*,x_{t+1}^*) = \sup_{\Pi(x_0^*)} \sum_{t=0}^\infty \beta^t F(x_t^*,x_{t+1}^*) = \max_{\Pi(x_0^*)} \sum_{t=0}^\infty \beta^t F(x_t^*,x_{t+1}^*)$$
    
    And let $\tau\in\mathds{N}$ fixed (but arbitrary).
\end{frame}

\begin{frame}
    The contribution of $x_{\tau}^*$ to the objective function lies within the following terms
    $$\beta^{\tau-1} F(x_{\tau-1}^*,x_{\tau}^*) + \beta^\tau F(x_\tau^*,x_{\tau+1}^*)$$
    with
    $$x_\tau^*\in\Gamma(x_{\tau-1}^*),\quad x_{\tau+1}^*\in\Gamma(x_{\tau}^*)$$
    All the other terms, do not have $x_\tau^*$ in them.
\end{frame}

\begin{frame}{Quick Quiz - 10 minutes}
    Note:
    \begin{align*}
        \beta^{\tau-1} F(x_{\tau-1}^*,x_{\tau}^*) + \beta^\tau F(x_\tau^*,x_{\tau+1}^*) &= \\
        \max_{x\in\Gamma(x_{\tau-1}^*),x_{\tau+1}^*\in\Gamma(x)} &\beta^{\tau-1} F(x_{\tau-1}^*,x) + \beta^\tau F(x,x_{\tau+1}^*)
    \end{align*}
    
    Why? Prove it. Hint: Go by contradiction.
\end{frame}

\begin{frame}
    
    If $x_\tau^*\in int\Gamma(x_{\tau-1}^*)$ and $x_{\tau+1}^*\in int\Gamma(x_{\tau}^*)$, then $x_\tau^*$ is a local maximizer of $$\beta^{\tau-1} F(x_{\tau-1}^*,x) + \beta^\tau F(x,x_{\tau+1}^*)$$, and if $F()$ is differentiable, then
    
    $$F'_2(x_{\tau-1}^*,x_\tau^*) + \beta F'_1(x_\tau^*,x_{\tau+1}^*) = 0$$
    
    Which is the Euler's Equation. $F'_i$ represents the derivative of $F$ with respect to the $i^{th}$ coordinate.
    
\end{frame}

\begin{frame}{Euler's Equation}

\begin{proposition}
    If $\{x_t^*\}_{t=0}^*$ is optimal for the initial value $x_0^*$, and $F()$ is differentiable, and if $x_t^*\in int \Gamma(x_{t-1}^*) \forall t\in\mathds{N}$ then  $$F'_2(x_{t-1}^*,x_t^*) + \beta F'_1(x_t^*,x_{t+1}^*) = 0\quad\forall t\in\mathds{N}$$ is a necessary condition for an interior optimizer.
\end{proposition}
    
    Note, \textbf{necessary} is not the same as \emph{sufficient}.
    
    Now let's go back to our...
    
\end{frame}


\begin{frame}
    
    \begin{figure}
        \centering
        \includegraphics[scale = 0.8]{cake.png}
    \end{figure}
    
\end{frame}

\begin{frame}
    We had
    \begin{align*}
      \sup_{\{x_t\}} \sum_{t=0}^\infty \beta^t \ln(x_t-x_{t+1})\\
      s.t.\quad x_{t+1}\in(0,x_{t})\\
      x_0\quad given
    \end{align*}
\end{frame}

\begin{frame}
    Is the objective function well defined?
    
    \vspace{0.5cm}
    
    \pause
    
    By definition $x_0>x_1>x_2>... > 0$, so $\exists x_\infty = \lim_{t\rightarrow\infty} x_t$, and therefore $\lim_{t\rightarrow\infty} x_t - x_{t+1} = x_\infty - x_\infty = 0$...
    
    \vspace{0.5cm}
    
    Why does $x_t$ converge? Quick Quiz $\rightarrow$ 5 minutes. \pause
    
    \vspace{0.5cm}
    
    Monotonic and bounded! we can use the monotone convergence theorem.
    
\end{frame}

\begin{frame}
    If $x_t$ is convergent, then $\exists T\in\mathds{N}$ such that for $t>T$ $x_t-x_{t+1} < 1$ or $\ln(x_t-x_{t+1}) < 0$.
    
    Let $S>T$,
    
    $$\sum_{t=0}^S \beta^t \ln(x_t-x_{t+1}) = \underbrace{\sum_{t=0}^T \beta^t \ln(x_t-x_{t+1})}_{\in\mathds{R}} + \underbrace{\sum_{t=T+1}^S \beta^t \ln(x_t-x_{t+1})}_{\text{decreasing in S}}$$
    
    So there exists a limit in $\mathds{R}\cup\{-\infty\}$.
    
\end{frame}

\begin{frame}
    Now
    \begin{align*}
        F(x_t,x_{t+1}) = ln(x_t-x_{t+1})
    \end{align*}
    Leads to:
    \begin{align*}
        \beta^{t-1}[\ln(x_{t-1}-x_t)+\beta\ln(x_t-x_{t+1})]
    \end{align*}
    And therefore the Euler equation is:
    \begin{align*}
        -\frac{1}{x_{t-1}-x_t}+\beta\frac{1}{x_t-x_{t+1}} = 0
    \end{align*}
    Note that $x_{t-1}-x_t = c_{t-1}$ so
    \begin{align*}
        -\frac{1}{c_{t-1}}+\beta\frac{1}{c_t} = 0 \quad\Rightarrow\quad c_t = \beta c_{t-1}\quad\Rightarrow\quad c_t = \beta^t c_0
    \end{align*}
\end{frame}

\begin{frame}
    Now note, $c_0 = x_0 - x_1$, and have no clue about $x_1$ just yet, so we need an extra condition for $x_1$.
    
    \vspace{0.5cm}
    
    Use the fact that $\sum_{t=0}^\infty c_t \leq x_0$... you cannot eat more than the cake!
    
    And $c_0 = x_0-x_1$, $c_1=x_1-x_2$, $c_2 = x_2 - x_3$ ... $c_T = x_T - x_{T+1}$, so $\sum_{t=0}^T c_t = x_0 - x_{T+1}$, let $T\rightarrow\infty$, then so $\sum_{t=0}^\infty c_t=x_0 - x_{\infty} \leq x_0$, and consider that $x_{\infty}\geq 0$.
    
    \vspace{0.5cm}
    
    Note now that if $\sum_{t=0}^\infty c_t < x_0$, then $c_t$ cannot be optimal, as there is cake left to be eaten!, so necessarily optimality implies $x_\infty = 0$, so the extra constraint is the \textbf{transversality condition}.
    
    \begin{align*}
        \lim_{T\rightarrow \infty} x_T = 0
    \end{align*}
    
\end{frame}

\begin{frame}

    \begin{align*}
        c_t &= \beta^t c_0\\
        x_t - x_{t+1} &= \beta^t c_0\\
        x_{t+1} &= x_t - \beta^t c_0\\
        x_{t+1} &= (x_{t-1}-\beta^{t-1}c_0) - \beta^t c_0\\
        &\vdots\\
        x_{t+1}&= x_0 - c_0 - ... - \beta^t c_0\\
        x_{t+1}&= x_0 - c_0 \frac{1-\beta^{t+1}}{1-\beta}\\
        x_{t}&= x_0 - c_0 \frac{1-\beta^{t}}{1-\beta}\\
        &\vdots \quad t\rightarrow \infty \\
        x_\infty &= x_0 - c_0 \frac{1}{1-\beta} = 0
    \end{align*}

\end{frame}

\begin{frame}
    
    And replacing for $x$, we have
    
    \begin{align*}
        x_0 - \frac{c_0}{1-\beta} &= 0\\
        x_0 - \frac{x_0-x_1}{1-\beta} & = 0\\
        x_1 & = \beta x_0
    \end{align*}
    
    And as $c_0 = x_0-x_1 = x_0 - \beta x_0 = (1-\beta)x_0$, then if an optimal exists, then it is $$c_t = \beta^t(1-\beta)x_0$$
    
\end{frame}

\end{document}
