\documentclass[answers]{exam}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}

\title{Problem Set \\ Linear Algebra}
\author{Paulo Fagandini}
\date{}

\begin{document}

\maketitle

\begin{questions}

\question Show that the sets $V_1=\{0_{\mathbb{R}^n}\}$ and $V_2=\mathbb{R}^n$ are vector subspace of $\mathbb{R}^n$.

\question Let $V=\{X\in\mathbb{R}^n|X_n=0\}$, show that $V$ is a vector subspace of $\mathbb{R}^n$.

\question Let $V_1$ and $V_2$ be both vector subspace of $\mathbb{R}^n$, show that $V_1\cap V_2$ is also a vector subspace of $\mathbb{R}^n$.
\begin{solution}
    If $V_1$ and $V_2$ are v.s. then for any $\hat{v}_1,\tilde{v}_1\in V_1$, $\hat{v}_1+\lambda \tilde{v}_1\in V_1$, and for any $\hat{v}_2,\tilde{v}_2\in V_2$, $\hat{v}_2+\lambda \tilde{v}_2\in V_2$, for any $\lambda\in\mathbb{R}$. Take any $\hat{v},\tilde{v} \in V_1\cap V_2$, and $\lambda\in\mathbb{R}$, then $v=\hat{v}+\lambda\tilde{v}$. If $\hat{v},\tilde{v}\in V_1\cap V_2$ then $\hat{v},\tilde{v}\in V_1$, so $v\in V_1$ because $V_1$ is v.s. Do the same for $V_2$. Then, $v\in V_1\cap V_2$, so $V_1\cap V_2$ is a v.s.
\end{solution}

\question Let $X_0\in\mathbb{R}^n$, and let $V_{X_0}=\{\alpha X_0 | \alpha\in\mathbb{R}\}$. Show that $V_{X_0}$ is a vector subspace $\mathbb{R}^n$.

\question Let $X_0$, $X_1$ $\in\mathbb{R}^n$. Show that $V_{X_0}\cap V_{X_1}\neq\{0_{\mathbb{R}^n}\}$ if and only if there is a scalar $\lambda\neq0$ such that $X_0=\lambda X_1$.

\question Let $X_1^t=(1,2,3)$ and $X_2^t=(4,5,6)$, check if $X=(10,11,12)$ is an element of $L\{X_1,X_2\}$.

\begin{solution}
    If $X\in L\{X_1,X_2\}$, there there are $\alpha$ and $\beta$ in $\mathbb{R}$ such that
    \begin{align*}
        \alpha\left(\begin{array}{c}
             1  \\
             2 \\
             3
        \end{array}\right) +
        \beta\left(\begin{array}{c}
             4  \\
             5 \\
             6
        \end{array}\right) =
        \left(\begin{array}{c}
             10  \\
             11 \\
             12
        \end{array}\right)
    \end{align*}
    
    This leaves the system:
    \begin{align*}
        a&+4\beta = 10\\
        2\alpha &+ 5\beta = 11\\
        3\alpha &+ 6\beta = 12
    \end{align*}
    
    Replacing the first one in the other equations:
    
    \begin{align*}
        \alpha&+\beta = 1\\
        2\alpha &+ 2\beta = 2
    \end{align*}

    So basically, for any $\alpha,\beta$ such that $\alpha+\beta=1$ the condition is satisfied. Replace back in the first equation. $\alpha+4\beta=10$, we get now $1+3\beta=10$ or $\beta=3$, and therefore $\alpha = -2$.

\end{solution}

\question Show that two vector sub space $V_1, V_2$ of $\mathbb{R}^n$ it holds that $dim(V_1\cap V_2)\leq \min\{dim(V_1),dim(V_2)\}$

\question Let the vector subspace $V_1=\{X\in\mathbb{R}^n|X_n=0\}$ of $\mathbb{R}^n$. Find $dim(V_1)$. Analogously do the same for the vector subspace $V_2=\{X\in\mathbb{R}^n|\sum_{i=1}^n a_iX_i=0\}$, given some $\alpha_i$s.

\question Show that for any $X\in\mathbb{R}^n$, $0\perp X$. Show also that if $X\in\mathbb{R}^n$ is such that $Z\perp X$, for any $X\in\mathbb{R}^n$, then $Z=0$.
\begin{solution}

    First part:
    \begin{align*}
        0\perp X\ \Leftrightarrow\ 0\cdot X = 0 \\
        0\cdot X = \sum 0\times x_i = 0
    \end{align*}
    Second part:
    \begin{align*}
        Z\cdot X= \sum z_i\times x_i = 0
    \end{align*}
    If this is true for any $\mathbb{R}^n$, then in particular is true for the vectors of the canonical basis of $\mathbb{R}^n$. Let $C^i$ represent each of these vectors, with 1 in the $i$th component, and 0 everywhere else. The inner product would be:
    \begin{align*}
        Z\cdot C_i = z_i\times c_i =z_i\times 1= z_i
    \end{align*}
    Doing the same for every vector in the canonical basis, we get that $z_i=0$ for every $i$, and therefore $Z=0$.
\end{solution}

\question Show that if $X\perp X_i$, with $i=1,2,...,k$, then $X\perp Y$, for any $Y\in L(\{X_1,X_2,...,X_k\})$.

\question Show that $\hat{X}=\frac{X}{||X||}$ is a unit vector, for $X\neq0$.

\question Consider the family of vectors $\mathcal{B}=\{e_1,e_2,...,e_n\}\subseteq\mathbb{R}^n$ where $e_i = (0,...,0,1,0,...,0)$. Show that $\mathcal{B}$ is a family of unit vector that are mutually perpendicular. Show also that for any $X\in\mathbb{R}^n$, it holds that $\sum_{j=1}^n (X\cdot e_j)e_j$.

\question Show that if $X,Y$ are two perpendicular vectors, different from zero, then $X$ and $Y$ are linearly independent.

\begin{solution}
   Let $X$ and $Y$ be linearly dependent, then $\exists$ $a,b\neq0$ such that $aX+bY=0$ or $X=\frac{-b}{a}Y$. 
    \begin{align*}
        X\cdot Y = \sum x_i\times y_i = \sum \frac{-b}{a}y_i\times y_i = \frac{-b}{a}\sum y_i^2
    \end{align*}
    But, as $Y\neq0$, then at least for one $i$, $y_i^2\neq 0$, and therefore $X\cdot Y\neq 0$, so the vectors are not perpendicular.
\end{solution}

\question Let $X_1,X_2,...,X_n$ be non zero vectors and orthogonal among them, show that $\{X_1,X_2,...,X_N\}$ as a basis of $\mathbb{R}^n$. If further, we assume that these vectors are unit vectors, show then that for any $X\in\mathbb{R}^n$ it holds that $X=\sum_{j=1}^n (X\cdot X_j)x_j$.

\question Show that the matrices, with the sum and scalar multiplication, is a vector subspace.

\question Given the matrix $$A=\left(\begin{array}{ccc}\alpha&0&0\\0&1&0\\0&0&4\end{array}\right)$$ Show that $A$ is invertible if and only if $\alpha\neq 0$.

\begin{solution}
    \begin{align*}
        \det(A) = 4\alpha
    \end{align*}
    $(A\ \text{invertible}\Rightarrow \alpha\neq 0)$ We know that $A$ is invertible if and only if $\det(A)\neq 0$, so given that $\det(A)=4\alpha$, we know that $4\alpha\neq0$, and therefore $\alpha\neq 0$.
    
    $(\alpha \neq 0 \Rightarrow A\ \text{invertible})$ Again, this is equivalent to $A\ \text{not invertible}\Rightarrow \alpha = 0$. We know that $A\ \text{not invertible}$ if and only if $\det(A)=0$, therefore $4\alpha=0$, or $\alpha=0$.
\end{solution}

\question Show that, for a given matrix $A$, the rank of $A$ is the same that the rank of its transpose.

\question Let $A$ be an upper triangular matrix. Show that its rank coincides with the number of non zero elements that lie on its diagonal.

\question Let $Y^t=(1,2,3,4)$, $X^t=(1,1,1,1)$, $X_2^t=(0,1,0,1)$ in $\mathbb{R}^4$. Find $proj_V(Y)$ with $V=L\{X_1,X_2\}$.

\question Let $$A=\left(\begin{array}{ccc}1&2&3\\2&\beta&4\\0&3&\alpha\end{array}\right)$$
Find conditions over $\alpha$ and $\beta$ such that $A$ is invertible.

\begin{solution}
    $\det(A)=\beta\alpha+0+18-0-12-4\alpha=\alpha\beta-4\alpha+6$. What is necessary is that $\det(A)\neq0$, therefore $\alpha\beta-4\alpha+6\neq0$. If $\alpha\beta-4\alpha+6=0$, then $\alpha(4-\beta)=6$ so for $A$ to be invertible, it is necessary that $\alpha\neq\frac{6}{4-\beta}$ or that $\beta=4$.
\end{solution}

\question Let $A\in\mathbb{R}^{n\times n}$, be such that its eigenvalues are different between them, and also different from zero. Let $V$ the matrix composed with the eigenvectors, that is first column of $V$ is the eigenvector associated to the first eigenvalue.

\begin{parts}
\part Show that $V^{-1}AV=D(\lambda)$, being $D(\lambda)$ the diagonal matrix whose elements are the eigenvalues of $A$.
\part Show that $A$ is invertible if and only if all its eigenvalues are different from zero.
\part Show that for any $n\in\mathbb{N}$, it holds that $A^n=V D(\lambda^n)V^{-1}$, where $D(\lambda^n)$ is the diagonal matrix with the eigenvalues of $A$ raised to the power of $n$.
\end{parts}

\question Let $A\in\mathbb{R}^n$ a diagonal matrix with values $\lambda_i$, $i=1,...,n$. Show that $det(A)=\prod_{i=1}^n \lambda_i$.

\question Show that the determinant of an upper triangular matrix is equal to the product of the elements on its diagonal.

\question Consider the following matrix $$A=\begin{pmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{pmatrix}$$.
\begin{parts}
    \part Find its eigenvalues and eigenvectors.
    \begin{solution}
        Find $\det(A-\lambda I)$ to solve the characteristic equation:
        \begin{align*}
            (A-\lambda I) &= \left(\begin{array}{ccc}
                                  1-\lambda & -1 & 0\\
                                  -1 & 2-\lambda & -1 \\
                                  0 & -1 & 1-\lambda
                                  \end{array}\right) \\
                                  \vspace{0.5cm}
            \det(A-\lambda I)&=(1-\lambda)^2(2-\lambda)-2(1-\lambda)\\
            &= (1-\lambda)\left[(1-\lambda)(2-\lambda)-2\right]\\
            &= (1-\lambda)\left[2-3\lambda+\lambda^2-2\right]\\
            &= (1-\lambda)\left[-3\lambda+\lambda^2\right]\\
            &= (1-\lambda)\lambda\left[-3+\lambda\right]
        \end{align*}
        
        So for the determinant to be zero, $\lambda = 0$, or $\lambda = 1$, or $\lambda = 3$. Let's find the eigenvectors associated to those eigenvalues.
        
        Start with $\lambda=0$
        
        \begin{align*}
            \begin{pmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{pmatrix}\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} = \begin{pmatrix} 0\\0\\0 \end{pmatrix}
        \end{align*}
        
        We obtain:
        \begin{align*}
            x_1-x_2&=0\\
            -x_1+x_2-x_3&=0\\
            -x_2+x_3&=0
        \end{align*}
        
        From where we obtain $x_1=x_2=x_3$, so the vector $(1,1,1)^T$ is the eigenvector associated to $\lambda=0$, in particular for $k=1$. Now lets look for the eigenvector associated to the eigenvalue $\lambda = 1$
        
            We obtain:
        \begin{align*}
            x_1-x_2&=x_1\\
            -x_1+x_2-x_3&=x_2\\
            -x_2+x_3&=x_3
        \end{align*}
        
        From where we get $x_2=0$ from the first equation, $x_1=-x_3$. Then the vector $(1,0,-1)^T$ would be an eigenvector associated to $\lambda = 1$. Finally, when $\lambda = 3$ we obtain:
        
        \begin{align*}
            x_1-x_2&=3x_1\\
            -x_1+x_2-x_3&=3x_2\\
            -x_2+x_3&=3x_3
        \end{align*}
        
        Which leads $x_2=-2x_1=-2x_3$, so $x_1=x_3$. Then the vector $(1,-2,1)^T$ would be the final eigen vector associated to the eigenvalue $\lambda=3$.
        
    \end{solution}

    \part Find $A^5$
    \begin{solution}
        Having all the eigenvalues and eigenvectors we can write the matrix decomposition:
        \begin{align*}
            A = \begin{pmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{pmatrix} = \begin{pmatrix}1 & 1 & 1 \\ 1 & 0 & -2 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix}0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3\end{pmatrix} \begin{pmatrix} \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\ \frac{1}{2} & 0 & -\frac{1}{2} \\ \frac{1}{6} & -\frac{1}{3} & \frac{1}{6} \end{pmatrix}
        \end{align*}
        And given that $A=VDV^{-1}$.
        \begin{align*}
            A^5 = \begin{pmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{pmatrix} &= \begin{pmatrix}1 & 1 & 1 \\ 1 & 0 & -2 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix}0 & 0 & 0 \\ 0 & 1^5 & 0 \\ 0 & 0 & 3^5\end{pmatrix} \begin{pmatrix} \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\ \frac{1}{2} & 0 & -\frac{1}{2} \\ \frac{1}{6} & -\frac{1}{3} & \frac{1}{6} \end{pmatrix}\\
            &= \begin{pmatrix}1 & 1 & 1 \\ 1 & 0 & -2 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix}0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 243\end{pmatrix} \begin{pmatrix} \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\ \frac{1}{2} & 0 & -\frac{1}{2} \\ \frac{1}{6} & -\frac{1}{3} & \frac{1}{6} \end{pmatrix}\\
            &=\begin{pmatrix}
                    41 & -81 & 40 \\
                    -81 & 162 & -81 \\
                    40 & -81 & 41
            \end{pmatrix}
        \end{align*}
        
    \end{solution}

\end{parts}

\question Let $A$ be a positive semidifinite matrix. Show that there is a matrix $R$ such that $A$ can be written as $A=R^tR$.

\begin{solution}
As $A$ is positive semidefinite, then we know there is a diagonal matrix with its eigenvalues, all positive, and an orthogonal matrix $V$ such that $A=VDV^t$. Define $H=D(\sqrt{\lambda})$, then $A=VHHV^t$, but as $H$ is also diagonal, $H=H^t$, so $A=VHH^tV^t=VH(VH)^t$. Let $R=VH$.
\end{solution}

\question Show that if $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is linear, then $f(0_{\mathbb{R}^n})=0_{\mathbb{R}^m}$.

\question Show that if $f,g:\mathbb{R}^n\rightarrow\mathbb{R}^n$ are linear functions, then $f+g$ is also linear.

\begin{solution}
    $f$ and $g$ linear, then for any $X,Y\in\mathbb{R}^n$ $f(X+Y)=f(X)+f(Y)$, $g(X+Y)=g(X)+g(Y)$, $f(\alpha X)=\alpha f(X)$, and $g(\alpha X)=\alpha g(X)$ for $\alpha \in \mathbb{R}$.
    
    Then, $[f+g](X+Y) = f(X+Y)+g(X+Y)=f(X)+f(Y)+g(X)+g(Y)=[f+g](X)+[f+g](Y)$.
    
    Also, $[f+g](\alpha X) = f(\alpha X) + g(\alpha X) = \alpha f(X) + \alpha g(X)=\alpha(f(X)+g(X))=\alpha [f+g](X)$
    
    Concluding, $f+g$ is linear.
\end{solution}

\question Show that if $f,g:\mathbb{R}^n\rightarrow\mathbb{R}^n$ are linear functions, then $f\circ g$ is also linear. Indeed, show that $[f\circ g]=[f][g]$.

\end{questions}

\end{document}

