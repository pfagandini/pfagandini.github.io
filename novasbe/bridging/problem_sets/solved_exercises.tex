\documentclass[answers]{exam}
\usepackage{inputenc} 
\usepackage{amsmath}                        % Para ecuaciones y demases
\usepackage{amsthm}                         % Para ecuaciones y demases
\usepackage{amssymb}                        % Para ecuaciones y demases
\usepackage{cancel}
\usepackage{setspace}
\usepackage{dsfont}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Some exercises}
\author{Paulo Fagandini\thanks{Any mistake in the solutions is of the exclusive my responsibility.}}
\date{}

\begin{document}

\maketitle

\begin{questions}

    \question Show that the set $A=\{(x,y)\in\mathds{R}^2:x>y\}$ is open in $\mathds{R}^2$.

    \begin{solution}
        Let $f(x,y)=x-y$. $f$ is a difference of continuous functions, then, it is continuous. The codomain of $f$ is open (given that $x>y\Rightarrow f(x,y)=x-y>0$)
        Finally, continuous functions take open sets to open sets, and therefore the domain must be open. Then $A$ must be open.
    \end{solution}

    \question Show that $B\subset U \subset \mathds{R}^2$ is closed in U $\Leftrightarrow$ $\forall \{x_n\}_{n\in\mathds{N}}\rightarrow\bar{x},\{x_n\}_{n\in\mathds{N}}\subset B,\bar{x}\in B$
        
        \begin{solution}
        $(\Rightarrow)$
        
        Let $\{x_n\}_{n\in\mathds{N}} \in B, \{x_n\}_{n\in\mathds{N}}\rightarrow\bar{x}$.
        
        Let $\bar{x}\in B^c$, given that $B$ is closed, then by definition $B^c$ is open. It follows that:  $$\exists \epsilon>0 \text{ tal que} B_\epsilon(\bar{x})\subset B^c$$
        
        Using the definition of converging sequence, then $\exists n\in\mathds{N}$ such that: $\parallel x_n-\bar{x}\parallel<\epsilon$
        
        Then, if we count the $n>N$ we obtain a convergent sequence with part of it in $B^c$, but by hypothesis, the sequence is contained in $B$, contradiction.
        
        $(\Leftarrow)$
        
        The sequence $\{x_n\}_{n\in\mathds{N}}\subset B$ converges to $\bar{x}\in B$. Assume that $B$ is not closed, then $B^c$ is not open, and therefore $\exists \bar{x}\in B^c$ such that $\forall \epsilon>0, B_\epsilon(\bar{x})$ has some elements of $B$.
        
        If you pick positive integers such that $\parallel x_n-\bar{x}\parallel <\frac{1}{n}$ with $x_n\in B$ then we have a convergent sequence with limit $\bar{x}$ and
        $\{x_n\}_{n\in\mathds{N}}\in B$ with $\bar{x} \in B^c$, contradiction.
        \end{solution}

    \question Given $U\subset\mathds{R}^n$, if the function $g:U\rightarrow\mathds{R}$ is continuous in $U$, then $\{x\in U: g(x)\geq0\}$ is closed in $U$. Show that you cannot change the \emph{then} with an \emph{if and only if} ( i.e. $\nLeftarrow$).
    \begin{solution}
        It is enough to show that $\{x\in U: g(x)\geq0\}$ closed in $U$ is not enough to show that $g:U\rightarrow\mathds{R}$ is continuous in $U$.
        Even further, it is enough to show some $g$ that satisfies $\{x\in U: g(x)\geq0\}$ closed in $U$.
        Take $U=[-3,3]\subset \mathds{R}^n$ such that $g(x)=-1$ when $x\in[-3,1)$ and $g(x)=1$ when $x\in[1,3]$. $\{x\in U: g(x)\geq0\}$ is closed in $U$, $U\subset\mathds{R}^n$ but $g$ is not continuous.
    \end{solution}

    \question Show: For $U\subset \mathds{R}$ compact and $f$ continuous, then $f:U\rightarrow\mathds{R}$ is uniformly continuous.
    \begin{solution}
        Let $B(x)$ an open ball in $U$, centered in $x$ with radius $\delta(x)/2$. $U$ is the union of a set of these open balls.
        For every point in the set, you can create one of these balls. However, as $U$ is compact we can choose a finite set of points that serve as center for these balls, such that:
        $$U=B(x_1)\bigcup B(x_2) \bigcup B(x_3)... \bigcup B(x_n)$$
        If you choose $delta$ as the lowest of the $\delta(x)/2$, we can be certain that we have satisfied the requirement (that the intersection is contained in the set).
        Now take $p$ and $q$ $\in U$, such that $d(p,q)<\delta$, for some $x_i$, you have that $p\in B(x_i)$. Then $d(p,x_i)<\delta(x_i)/2$
        However, $$d(p_i,q)\leq d(x_i,q)+d(p,q)<\delta(x_i)/s+\delta\leq \delta(x_i)$$
        Then wen use $\delta=\min\{\delta(x_i)/2\}_{i\in\{1,...,n\}}$.
        $$d(x_i,p)+d(x_i,q)<\delta(x_i)$$
        Now using point continuity, given $\delta(x_i)$ we choose it, and we have $d(f(x_i),d(p))<\frac{\epsilon}{2}$ y $d(f(x_i),f(q))<\epsilon/2$. $$d(f(p),f(q))\leq d(f(x_i),f(p))+d(f(x_i),f(q))<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$$
    \end{solution}

\question Show that $\emptyset$ and $\mathds{R}^n$ are convex.
    \begin{solution}
    $\emptyset\rightarrow$ You cannot show that is not convex, as you cannot pick two elements in the set such that do not satisfy convexity.
    $\mathds{R}^n\rightarrow$ Sea a y b $\in \mathds{R}^n, t\in[0,1]$
    $ta+(1-t)b$, because of the axioms of $\mathds{R}^n$ $ta$ belongs to $\mathds{R}^n$, and the summation as well, then it is convex (and a vector space!).
    \end{solution}

\question Given A and B convex, subsets of $\mathds{R}^n$, show that: $$A+tB:=\{a+tb:a\in A, b\in B\}$$ is convex $\forall t\in\mathds{R}$.
    \begin{solution}
        Let $a_i\in A$, $b_i\in B$ y $z_i=a_i+tb_i\in A+tB$ $i=1,2$.
        Let's show that $\{\lambda z_1+(1-\lambda)z_2\}\in A+tB$ $\lambda \in (0,1)$ $\forall t\in \mathds{R}$, $z_1, z_2 \in A+tB$
        
        We know that $$\lambda z_1+(1-\lambda)z_2=\lambda a_1+\lambda tb_1+(1-\lambda)a_2+(1-\lambda)tb_2=\underbrace{\lambda a_1+(1-\lambda)a_2}_{\in A, convex}+t\underbrace{\lambda (b_1+(1-\lambda)b_2)}_{\in B, convex}$$
        Then, it belongs to the set.
    \end{solution}

\question An arbitrary intersection of convex sets is convex.
    \begin{solution}
    Let a and b elements in the intersection of convex sets, then $(\lambda a+(1-\lambda)b)$ belongs to each set, as these are convex. If it belongs to all, then it also belongs to the intersection. Then, the intersection is convex.
    \end{solution}
    
\item Show that a vector space is convex.
    \begin{solution}
        By definition, a vector space contains elements that can be added up and multiplied by a scalar. In particular, convex combinations of its elements belong to the same vector space.
        
        Let $a$ and $b$ belong to a vector space, $\lambda a$ and $(1-\lambda)b$ are well defined and belong to the vector space as $\lambda\in\mathds{R}$, and particularly $\lambda \in(0,1)$.
        
        As the summation is closed in the vector space then the summation $\lambda a+(1-\lambda)b$ belongs to the vector space, concluding the proof.
    \end{solution}

\question $C\subset\mathds{R}^n$ convex $\Rightarrow$ The closure is convex.
    \begin{solution}
    If $C$ is closed, then it is trivial. Let $C$ be not closed.
    
    Let the closure to be not convex, while $C$ is convex.
    
    Let $a,b\in \bar{C} \backslash C$ $\lambda \in (0,1)$, $\lambda a+ (1-\lambda)b \not\in \bar{C}$
    Then $\exists \{a_n\}_{n\in\mathds{N}}, \{b_n\}_{n\in\mathds{N}} \subset C$ such that $\{\lambda a_n+(1-\lambda)b_n\}\in C\forall n$ A linear combination is continuous, so if I get close to $a$ and $b$, the linear combination of these elements in the sequence must go through the closure, but $C$ is convex, then contradiction.
    \end{solution}
    

\question There is a market with $J$ assets traded by subjects that maximize their wealth. Each subject can create a portfolio $\theta=(\theta_1,...,\theta_J)\in\mathds{R}^{J}$, where $\theta_j>0$ means that the subject bought $\theta_j$ units of asset $j$, while $\theta_j<0$ means that the subject promised in the future to pay the market value of $\theta_j$ units of asset $J$ (or, \emph{shorted} $\theta_j$ units of the asset $j$). The value of each unit of asset $j$ is given by $q_j\geq 0$. Assume there is uncertainty regarding the future price of the assets. Consider that there are $S$ different possible states of nature. On state $s\in\{1,...,S\}$ the asset $j\in\{1,...,J\}$ has a value of $\mathds{R}_{s,j}$ per unit. In sum, a portfolio $\theta\in\mathds{R}^J$ will be worth, for a specific state of nature $s\in\{1,...,S\}$ $\sum_{j=1}^{J}R_{s,j}\theta_j$.

As said, the subjects try to maximize their wealth, it is expectable that there are no positions that could generate unlimited wealth without taking some risks. Formally, let's say that \emph{there is no arbitrage} if there is no portfolio $\theta\in\mathds{R}^J$ such that $\sum_{j=1}^Jq_k\theta_j\leq0$ and, for each state of nature $s\in\{1,...,S\}$, $\sum_{j=1}^J R_{s,j} \theta_j\geq0$, with at least one of the inequalities being strict. Put in words, there is no way of (i) get more money today without sacrificing (expected) future wealth; or (ii) without paying nothing today increasing future wealth.

The following steps are suggested:

\begin{parts}

\part Let  \begin{align*}
        A=\begin{pmatrix}
            -q_1 & \ldots & -q_J \\
            R_{1,1} & \ldots & R_{1,J} \\
            \vdots &  & \vdots \\
            R_{S,1} & \ldots & R_{S,1} \\
          \end{pmatrix}
    \end{align*}
    
    Show that, without aribtrage, the set $C:=\{z\in\mathds{R}^{S+1}:\exists \theta\in\mathds{R}^J,z=A\theta\}$ is disjoint with $C_\epsilon:=\{z\in\mathds{R}^{S+1}_{+}:\parallel z\parallel \in [\epsilon,2]\}$, $\epsilon>0$.
    
    \begin{solution}
        $C:=$\begin{center}
                $z=\begin{pmatrix}
                \sum_{i=1}^{J}-q_i\theta_i \\
                \sum_{i=1}^{J}R_{1i}\theta_i\\
                \vdots \\
                \sum_{i=1}^{J}R_{si}\theta_i \\
                \end{pmatrix}$
            \end{center}
              
            \begin{align*}
                \rightsquigarrow\quad z_1&=-q_1\theta_1-q_2\theta_2-...-q_J\theta_J\\
                &\vdots\\
                z_i&=R_{i1}\theta_1+R_{i2}\theta_2-...+R_{iJ}\theta_J\\
                &\vdots\\
                z_s&=R_{s1}\theta_1+R_{s2}\theta_2-...+R_{sJ}\theta_J
            \end{align*}
            
        If there is no arbitrage then:
    
        \begin{align*}
            \nexists \theta \in \mathds{R}^J\text{ such that }\sum_{j=1}^{J} q_j\theta_j\leq 0 \text{ and } s\in\{1...S\} \text{ }\sum_{j=1}^{J}R_{sj}\theta{j}\geq 0
        \end{align*}
        
        If you could find some $\theta$ such that happens, then $z\in \mathds{R}_{+}^{s+1}$. As it doesn't happen, it can be zero, or with some negative coordinate, then $z\in\mathds{R}^{s+1}\backslash\mathds{R}_{+}^{s+1}$. It follows that $C_\epsilon \cap C=\emptyset$ as $C_\epsilon \subset \mathds{R}_+^{s+1}$ and therefore they are disjoint.
    
    \end{solution}
    
    \part Show that $C$ and $C_\epsilon$ are convex, non empty and closed. Show that $C_\epsilon$ is compact.
    
    \begin{solution}
    
        The empty set case is trivial because $0 \in C$ and given $z\in C_\epsilon$, in particular a vector such that $\{c\in\mathds{R}_{+}^{s+1}:0\leq\parallel c\parallel\leq 2\}\Rightarrow \sqrt{\sum_{1}^{s+1}(c_i)^2}\leq 2$
            
            \begin{align*}
                \sum_{i=1}^{s+1}(c_i)^2 \leq 4
            \end{align*}
            
        For example $c_j=2$ and all the others $0$ satisfies the requirement.
        
        \textbf{Convexity},
        
        The set $C_\epsilon$ is a closed ball in $\mathds{R}^{S+1}_{+}$, that is a convex set. Then $C_\epsilon$ is convex.
        
        $C$: Let $a$ and $b$ $\in C$, $\lambda \in [0,1]$
        
            \begin{align*}
                \lambda a+(1-\lambda)b&=\lambda A\theta_c+(1-\lambda)A\theta_b\\
                &=A\theta_b+\lambda(A\theta_c-A\theta_b)=A[\theta_b+\lambda(\theta_c-\theta_b)]\\
                &=A\underbrace{(\theta_b(1-\lambda)+\lambda\theta_c)}_{\in\mathds{R}^{J}}\Rightarrow\exists\theta\in\mathds{R}^{J}\text{ such that the set is convex}
            \end{align*}
            
        \textbf{Closedness},
        
        $C_\epsilon$ is closed in $\mathds{R}_+^{s+1}$. It is enough to show that $\mathds{R}_+^{s+1}$ is open if we exclude $C_\epsilon$, that is, to show that $\{z\in\mathds{R}_+^{s+1}:\parallel z\parallel<2\}$, but the function $\parallel\cdot\parallel$ is continuous, and its codomain in the set is $(2,+\infty)$, then the set is open. By definition of a closed set, $C_\epsilon$ is closed.
        
        $C$ is closed, because $z$ is a linear transformation of a vector space. The linear transformation of a vector space is itself a vector space, and a vector space is always closed.
        
        That $C_\epsilon$ is compact is trivial, as the norm of all of its element is bounded by 2. Then, as we have shown, $C_\epsilon$ is closed and bounded, then it is compact.
    
    \end{solution}
        
    \part Show that there is $p>>0$ such that $pz\leq0$, for each $z\in C$. [Hint: Lookout the \textbf{hyperplane separation theorem}]
        
        \begin{solution}
    
            From the previous result, we can apply the hyperplane separation theorem. In particular we have $$pa<c<pb\quad \forall (a,b) \in C \times\ C_\epsilon,\text{ fore some }c\in\mathds{R}$$
            
            Where we can chose $p$.
            
            Of curse we cannot set $p=0$ as $0<0$ is false. The elements in $C_\epsilon$ have at least one positive coordinate. Then, we can bound it from below with 0. However 0 is feasible in $C$, and we know that $pz\leq0$. As $z$ has some negative coordinate, the it could be that if $p$ would have negative coordinates, we could obtain something ``greater or equal'' than 0. Finally, as $p$ cannot be 0, then $p>>0$.
        
        \end{solution}
    
    \part  Show that $pz=0$ for each $z\in C$ (remember that $C$ is a vector subspace).
    
        \begin{solution}
            Let's assume that $pz<0$. If that is true, then $p(-z)>0$, but as $C$ is a vector subspace, $-z$ must belong to the vector subspace, but $pz<0$  $\forall z\in C$. Contradiction. The only possibility is that $pz=0$.
        \end{solution}
    
    \part Conclude the proof.
    
        \begin{solution}
            Given that $pz=0$ and that $p>>0$ then:
            \begin{align*}
                pz&=0\\
                -p_1\sum_{j=1}^{J}q_j\theta_j+p_2\sum_{j=1}^{J}R_{1,j}\theta_j+\ldots+p_{s+1}\sum_{j=1}^{J}R_{S,j}\theta_j&=0\\
                p_2\sum_{j=1}^{J}R_{1,j}\theta_j+\ldots+p_s+1\sum_{j=1}^{J}R_{S,j}\theta_j&=p_1\sum_{j=1}^{J}q_j\theta_j\\
                \frac{p_2}{p_1}\sum_{j=1}^{J}R_{1,j}\theta_j+\ldots+\frac{p_{s+1}}{p_1}\sum_{j=1}^{J}R_{S,j}\theta_j&=\sum_{j=1}^{J}q_j\theta_j
            \end{align*}
            If we redefine, $\frac{p_{j}}{p_1}=\gamma_{j-1}$ then we have:
            \begin{align*}
                \gamma_1\sum_{j=1}^{J}R_{1,j}\theta_j+\ldots+\gamma_S\sum_{j=1}^{J}R_{S,j}\theta_j=\sum_{j=1}^{J}q_j\theta_j
            \end{align*}
            The this is equivalent to:
            \begin{align*}
                \gamma^T R \theta&=q^T\theta\\
                \gamma^T R&=q^T
            \end{align*}
            The for each element $i$ we have that:
            $$\sum_{j=1}^J \gamma_i R_{i,j}=q_i$$
        \end{solution}
    
\end{parts}

    \vspace{1cm}

    For the next questions, consider that a function is said to be \textbf{quasiconcave} if $f(\lambda x_1+(1-\lambda)x_2)\geq \lambda f(x_1)+(1-\lambda)f(x_2)$

    \question Show that any function $f:\mathds{R}^n\rightarrow\mathds{R}$ as $f(x)=ax+b$, where $a\in\mathds{R}^n$ and $b\in\mathds{R}$, is quasiconcave.
        \begin{solution}
        \begin{align*}
            f(\lambda x_1+(1-\lambda)x_2)&=a\lambda(x_1-x_2)+f(x_2)+\lambda b-\lambda b\\
            &=\lambda f(x_1)-\lambda f(x_2)+f(x_2)\\
            &=\lambda f(x_1)+(1-\lambda)f(x_2)\\
            \Rightarrow f(\lambda x_1+(1-\lambda)x_2)&=\lambda f(x_1)+(1-\lambda)f(x_2)\\
            \text{si, } f(x_1)>f(x_2)\\
            \geq f(x_2)=\min{f(x_1),f(x_2)}\\
            \text{si, } f(x_1)<f(x_2)\\
            \geq f(x_1)=\min{f(x_1),f(x_2)}\\
            \Rightarrow f(\lambda x_1+(1-\lambda)x_2)&\geq\min{f(x_1),f(x_2)}
        \end{align*}
    \end{solution}

    \question A monotone function (increasing or decreasing) is always quasiconcave.

        \begin{solution}
        Having $x_1\geq x_2$ is clear that $\lambda x_1+(1-\lambda) x_2\geq x_2$, Then:
        \begin{align*}
            f(\lambda x_1+(1-\lambda) x_2)\geq f(x_2)\text{, if $f$ is increasing.}\\
            \Rightarrow f(x_2)=\min\{f(x_1),f(x_2)\}\\
            f(\lambda x_1+(1-\lambda) x_2)\geq f(x_1)\text{, if $f$ is decreasing.}\\
            \Rightarrow f(x_2)=\min\{f(x_1),f(x_2)\}\\
        \end{align*}
        In both cases $f(\lambda x_1+(1-\lambda)x_2)\geq\min\{f(x_1),f(x_2)\}$, then $f$ is quasiconcave.
        
        \end{solution}

    \question Any concave function is quasiconcave.
    \begin{solution}
        If:
        \begin{align*}
            f(x_1)<f(x_2)\Rightarrow \lambda f(x_1)+(1-\lambda)f(x_2)\geq\lambda f(x_1)+(1-\lambda)f(x_1)=f(x_1)=\min\{f(x_1),f(x_2)\}
        \end{align*}
        The proof in the opposite case follows in the same way.
    \end{solution}
    
    \question\label{q:uesful} Given function $f:U\subset\mathds{R}^n\rightarrow \mathds{R}$, where $U$ is convex, $f$ is quasiconcave in $U$ if and only if, for each $a$ in $\mathds{R}$ the set $U_a=\{x\in U:f(x)>a\}$ is convex.
    
        \begin{solution}

        $(\Rightarrow)$ hypothesis: 
        
        $f$ is quasiconcave in $U, a\in\mathds{R}$, for when $U=\emptyset$, is trivial. Let $U\neq\emptyset$.
        
        Let $x_1, x_2\in U$ $a\in\mathds{R}$, $\lambda \in(0,1)$.
        \begin{align*}
            z_\lambda&=\lambda x_1+(1-\lambda)x_2 \quad \in U\text{ , because $U$ is convex}\\
            \text{using $f$'s quasiconcavity}\\
            f(\lambda x_1+(1-\lambda)x_2)&\geq\min\{f(x_1),f(x_2)\}\rightarrow\quad\text{(easy to see by the def. of the set)}\\
            \min\{f(x_1),f(x_2)\}&>a\quad\text{then}\\
            f(\lambda x_1+(1-\lambda)x_2)&>a\Rightarrow f(\lambda x_1+(1-\lambda)x_2)\in U_a\\
            \Rightarrow U_a\text{ is convex}
        \end{align*}
        $(\Leftarrow)$ Now the hypothesis is: $U_a$ is convex for each $a\in\mathds{R}$.
        Let $x_1,x_2\in U$, $\{x_1,x_2\}\subset U_{\min\{f(x_1),f(x_2)\}}$ then for each $\lambda \in (0,1)$
        \begin{align*}
            \lambda x_1+(1-\lambda)x_2\quad \in \quad \min\{f(x_1),f(x_2)\}\\
            \Rightarrow f(\lambda x_1+(1-\lambda)x_2)\geq\min\{f(x_1),f(x_2)\}\quad\forall \lambda\in(0,1)
        \end{align*}
        Concluding the proof.

        \end{solution}

        \question Given $(\alpha,\beta)>>0$, $(x,y)\in\mathds{R^2}_+$, the function $f(x,y)=x^\alpha y^\beta$ is strictly quasiconcave.
        
            \begin{solution}
            Using the result in exercise (\ref{q:uesful}), if $a=0$, then it is easy to see that $f$ is quasiconcave. ($\mathds{R}^2_+$ is convex).
            \end{solution}

        \question Given $a\in\mathds{R}^n$, $f(x)=-\parallel x-a\parallel$ is strictly quasiconcave.

    \begin{solution}
            Let $x,y\in\mathds{R}^n$
            \begin{align*}
                z&=\lambda x+(1-\lambda)y\\
                g(x)&=-f(x)\\
                g(z)&=\parallel \lambda x+(1-\lambda)y-a\parallel\\
                &=\parallel\lambda(x-a)+(1-\lambda)(y-a)\parallel\\
                &\leq\lambda \parallel x-a\parallel+(1-\lambda)\parallel y-a\parallel
            \end{align*}
            Assume an $x$ further from $a$ than from $y$.
            \begin{align*}
                g(z)&<g(x)\\
                g(\lambda x+(1-\lambda)y)&<g(x)\\
                \parallel \lambda x+(1-\lambda)y-a\parallel &< \parallel x-a\parallel \quad /-1\\
                -\parallel \lambda x+(1-\lambda)y-a\parallel &> -\parallel x-a\parallel\\
                f(z)&>f(x)
            \end{align*}
            Because we assumed $x$ further from $a$ than from $y$,
            \begin{align*}
                g(x)=\max\{g(x),g(y)\}\\
                \Rightarrow f(x)=\min\{f(x),f(y)\}
            \end{align*}
            Then $f(z)>\min\{f(x),f(y)\}$, concluding the proof.
        \end{solution}

\section{Bonus Track, Proof fo the Caratheodory's Theorem}

\begin{definition}
    The covenxhull of a set $S$ is the set that contains all the convex combinations of a finite number of elements of $S$.
\end{definition}

If some $x\in\mathds{R}^d$ is in the convexhull of $P$ ($co(P)$), then, there is a subset $P'\subset P$ that has $d+1$ or less elements such that $x$ is in the $co(P)$.

\begin{solution}

Let $x\in co(P)$. Then $x$ is a convex combination of a finite number of elements of $P$: $$x=\sum_{j=1}^{k}\lambda_j x_j$$ where each $x_j\in P$, each $\lambda_j\geq0$ and it holds that $$\sum_{j=1}^{k}\lambda_j=1$$ Assume that $k>d+1$ (if not, there is nothing to prove). Then, the elements $(x_2-x_1),...,(x_k-x_1)$ are linearly dependent, that is there are scalars $\mu_2,...,\mu_k$ such that:

\begin{align*}
    \sum_{j=2}^{k}\mu_j(x_j-x_1)&=0\\
    \sum_{j=2}^{k}\mu_j x_j-\sum_{j=2}^{k}\mu_jx_1&=0\\
    \sum_{j=2}^{k}\mu_j x_j&=\sum_{j=2}^{k}\mu_jx_1\\
\end{align*}

If we define $$\mu_1:=-\sum_{j=2}^{k}\mu_j$$

\begin{align*}
    \sum_{j=2}^{k}\mu_j x_j=\sum_{j=2}^{k}\mu_jx_1\quad/+\mu_1x_1\\
    \sum_{j=1}^{k}\mu_j x_j=x_1\sum_{j=1}^{k}\mu_j\\
\end{align*}

And noting that:

\begin{align*}
    x_1\sum_{j=1}^{k}\mu_j=\mu_1x_1+\sum_{j=2}^{k}\mu_j x_1=-x_1\sum_{j=2}^{k}\mu_j+x_1\sum_{j=2}^{k}\mu_j=x_1\left[-\sum_{j=2}^{k}\mu_j+\sum_{j=2}^{k}\mu_j\right]=0
\end{align*}

Then $$\sum_{j=1}^{k}\mu_j x_j=x_1\sum_{j=1}^{k}\mu_j=0$$

\begin{align*}
    \sum_{j=1}^{k}\mu_j x_j=0\\
    \sum_{j=1}^{k}\mu_j=0
\end{align*}

And not all the $\mu_j$ are zero. Then, at least some $\mu_j>0$. It follows that,

\begin{align*}
    x=\sum_{j=1}^{k}\lambda_jx_j-\alpha\sum_{j=1}^{k}\mu_jx_j=\sum_{j=1}^{k}(\lambda_j-\alpha\mu_j)x_j
\end{align*}

for some $\alpha\in\mathds{R}$. In particular, the inequality will hold if $\alpha$ is defined as,

\begin{align*}
    \alpha:=\min_{1\leq j\leq k}\left\{\frac{\lambda_j}{\mu_j}:\mu_j>0\right\}=\frac{\lambda_i}{\mu_i}
\end{align*}

Note that $\alpha>0$ and for $j$ between $1$ and $k$, $$\lambda_j-\alpha\mu_j\geq 0$$ in particular $\lambda_j-\alpha\mu_j=0$, by definition of $\alpha$. Then $$x=\sum_{j=1}^{k}(\lambda_j-\alpha\mu_j)x_j$$ where each $(\lambda_j-\alpha\mu_j)$ is non negative, adds ip to 1 and even further, $\lambda_i-\alpha\mu_i=0$. In other words $x$ is a convex combination of at the most $k-1$ elements of $P$.

This process can be repeated until $x$ is represented as a convex combination of at the most $d+1$ elements of $P$.

\end{solution}

\end{questions}
\end{document} 