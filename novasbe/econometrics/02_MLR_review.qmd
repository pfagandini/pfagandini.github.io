---
title: "Review for lecture SLR"
format: html
---

## Matrix/vector SLR

The original equation:

$y_i=\beta_0+\beta_1 x_i + u_i$

This holds for $i=1,2,...,n$ therefore you can write

$$
\begin{pmatrix}y_1\\ y_2 \\ y_3 \\ \vdots \\ y_n\end{pmatrix} =
\beta_0\begin{pmatrix} 1 \\ 1 \\ 1 \\ \vdots \\ 1\end{pmatrix} +
\beta_1\begin{pmatrix}x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n\end{pmatrix} +
\begin{pmatrix}u_1 \\ u_2 \\ u_3 \\ \vdots \\ u_n\end{pmatrix}
$$

This is the same as:
$$
\begin{pmatrix}y_1\\ y_2 \\ y_3 \\ \vdots \\ y_n\end{pmatrix} =
\begin{pmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \\ \vdots \\ 1 & x_n\end{pmatrix} \begin{pmatrix}\beta_0 \\ \beta_1\end{pmatrix}+
\begin{pmatrix}u_1 \\ u_2 \\ u_3 \\ \vdots \\ u_n\end{pmatrix}
$$

On the order of appearece, this is equivalent to the following expression:

$$
Y = X\beta + u
$$

Where $Y$ and $u$ are the first and last vectors, $X$ is the matrix and $\beta$ is the vector of $\beta$s.

Why is this useful?

Remember that the sum of squares is the inner product $<u,u>=u'u=\sum u_i^2$. Then:

$$
\begin{aligned}
u &= Y - X\beta\\
\sum u_i^2=u'u &= \left(Y-X\beta\right)'\left(Y-X\beta\right)\\
&= Y'Y - Y'X\beta - \beta'X' Y + \beta' X'X \beta
\end{aligned}
$$

Note that $Y'X\beta$ has dimension $1$ ($1\times n$ times $n\times 2$ times $2\times 1$), and also that $\left(Y'X\beta\right)'=\beta' X' Y$, but a scalar (a vector of dimension 1 is actually a scalar, a number) trasposed is the same number and therefore $Y'X\beta=\beta'X'Y$. Finally, we can write:

$$
\begin{aligned}
\sum u^2=u'u&= Y'Y - 2 X'Y\beta + \beta' X'X \beta
\end{aligned}
$$

To minimize the squared residuals, use the first order condition to obtain:

$$
\begin{aligned}
-2X'Y + 2X'X\beta &= 0\\
\beta = (X'X)^{-1}X'Y
\end{aligned}
$$

Where obviously implies that $X'X$ must have full rank (no linearly dependent columns) to be invertible, which implies $X$ must have no lineraly dependent columns.

::: {.callout}

## Note about $\frac{\partial \beta' X'X\beta}{\partial \beta}$

Note that $X\beta=\hat{y}$ and therefore $\beta' X'X\beta=\hat{y}'\hat{y}=\sum \hat{y}^2$ or $\sum(\beta_0 + \beta_1 x_1)^2$. The derivative with respect to $\beta_0$ is $\sum 2 \hat{y}\cdot1$ and the derivative with respect to $\beta_1$ is $\sum 2 \hat{y} x$. You can factor out the 2, and note that $\begin{pmatrix}1 &x\end{pmatrix}'\hat{y} = \begin{pmatrix}\sum \hat{y}\cdot 1 \\ \sum \hat{y} x \end{pmatrix}$. Finally recall that $\hat{y}=X\beta$, and $\begin{pmatrix}1 &x\end{pmatrix}'=X'$ to obtain $2X'X\beta$.

:::

## MLR

This can trivially be generalized to $k$ regressors, just defining $X$ as:

$$
\begin{aligned}
X =
\begin{pmatrix}
    1 & x_1^1 & x_2^1 & x_3^1 & \dots & x_k^1 \\
    1 & x_1^2 & x_2^2 & x_3^2 & \dots & x_k^2 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_1^n & x_2^n & x_3^n & \dots & x_k^n
\end{pmatrix}\quad\quad
\beta =
\begin{pmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \vdots \\ \beta_k
\end{pmatrix}
\end{aligned}
$$

## $\beta_j$ as a regression of $y$ on the residuals of $x_j$ on $x_{-j}$

Let $$y=\beta_0+\sum \beta_i x_i + u$$ Running the regression $x_j = \gamma_0 + \sum_{i\neq j} \gamma_i x_i + r_j$ to obtain $r_j = x_j - \gamma_0-\sum_{i\neq j} \gamma_i x_i$.

Multiply the first equation by $r_j$ and sum 

$$\sum r_j y = \sum \beta_0 r_j + \sum \beta_i x_i r_j + \sum u r_j$$

Remember that $E[r_j]=0$ and that $r_j$ is orthogonal to $x_{-j}$ and therefore the previous expression reduces to:

$$\sum r_j y = \beta_j \sum  x_j r_j + \sum u r_j$$

Recall that $r_j=x_j-\gamma_0 - \sum_{i\neq j} \gamma_i x_i$, that is a linear combination of $x$s, and $u$ is orthogonal to all of them. The constant is dealt with $E[u]=0$.

This leaves us with:

$$\sum r_j y = \beta_j \sum  x_j r_j$$

Now consider the original regression of $x_j$ on $x_{-j}$, multiply both sides by $r_j$ to obtain:

$$r_jx_j = \gamma_0r_j + \sum_{i\neq j} \gamma_i x_i r_j + r_j ^2$$

Again, $r_j$ is orthogonal to all the $x_{-j}$ and therefore the second term would dissapear when adding up. The first term dissapears because $E[r_j]=0$, and therefore summing up on both sides we get:

$$\sum r_j x_j = \sum r_j^2$$

Which we can replace in the previous equation to obtain:

$$\sum r_j y = \beta_j \sum r_j^2$$

And finally

$$\beta_j = \frac{\sum r_j y}{\sum r_j^2}$$