---
title: "Introduction to Probability Theory"
author: "Paulo Fagandini"
institute: "Lisbon Accounting and Business School – Polytechnic University of Lisbon"
format: 
  revealjs:
    theme: default
    slide-number: true
    cross-ref: true
    incremental: true
    math: 
      method: mathjax
    transition: slide
    footer: "Statistics I"
    from: markdown+emoji
    code-block-font-size: 0.75em
    progress: true
editor: source
execute:
  enabled: true
engine: knitr
---

```{r}
library(lubridate)
```

# Probability, background

## The concept

Consider the following scenario:

:::{.fragment}

:briefcase: **Investor**: What’s the probability this startup will succeed?

:::

:::{.fragment}

:bar_chart: **Analyst**: Hard to say—every startup is different.

:::

:::{.fragment}

:briefcase: **Investor**: But if you had to guess, based on similar cases?

:::

:::{.fragment}

:bar_chart: **Analyst**: Maybe 1 in 3 succeed under these conditions.

:::

:::{.fragment}

:briefcase: **Investor**: So, would you bet on it?

:::

:::{.fragment}

:bar_chart: **Analyst**: Yes, I would.

:::

:::{.fragment}

:briefcase: **Investor**: Even if the odds aren’t great?

:::

:::{.fragment}

:bar_chart: **Analyst**: I believe this one has what it takes.

:::

## The concept

Here we can define probability in terms of **frequency of occurrence,** *i.e.* as a percentage of successes in a moderately large number of similar situations.

This is the most natural and traditional way of thinking about probability.

. . .

* Regarding a fair coin :coin: we could say: "with probability 50% the coin lands on heads" meaning "roughly half of the time."

. . . 

But, what if this company belongs to a completely novel market sector?

## The concept

There might be situations where the frequency concept is not adequate, because it might refer to a one-time event. These are **subjective beliefs.**

* A company is recruiting a new CEO, and a board member says: 

  "I believe there's a 90% chance that our chosen candidate will be an effective CEO."


## The concept

It might seem easy to disregard the second case as unscientific or useless. However, many times people need to make decisions under uncertainty with not enough data (or no data at all!) about previous realizations of the specific event.

Beliefs allow the decision maker to, well, make *some* decision, at least consistently.


##

* What's the difference between both situations?
* What do they have in common?

## 

::: {.fragment style="text-align: center; font-size: 3em;"}
**Uncertainty**
:::

##

# A refresh on Set Theory

## Sets and elements

A **set** is a collection of objects, which are **elements** of the set.

::: {.callout}

## Definition
Let $S$ represent a set, and $s$ an  element of that set, we write $s\in S$ to mean $s$ **belongs** to $S$.

If $s$ does not belong to $S$, we write $s\notin S$.
:::

. . .

:::{.callout}
## Definition

If a set $S$ does not have any element, then it is the **empty** set, denoted by $\emptyset$.
:::

## How to write down a Set

There are several ways to specify a set.

By extension, or as a list:

* If a set $S$ has a finite number of elements ($x_i\in S$) we can write it like this: $$S=\{x_1, x_2, ..., x_n\}$$

* If a set $S$ has an infinite (but countable) elements ($x_i\in S$) we can write it like: $$S=\{x_1, x_2, ...\}$$

## How to write down a Set

By describing the property ($P$) that $x$ must satisfy to be included in $S$: $$S= \{x|x \text{ satisfies } P\}$$ in this case $|$ reads as *such that.* For example $$S=\{x\in\mathbb{R}|x>=0\}$$ to describe the non-negative real numbers.

This example is special, as the positive real numbers cannot be written down as a a list. In this case the interval $[0,\infty)$ is an **uncountable** set.

## More definitions

:::{.callout}

## Definition

If $\forall x\in S$ it is also true that $x\in T$, then we say that $S$ is a **subset** of $T$, and we write it like $S\subseteq T$.

:::

. . .

:::{.callout}

## Definition

If $S\subseteq T$ and at the same time $T\subseteq S$ then we say that $S$ and $T$ are **equal**, and we write it $S=T$.

:::

. . .

:::{.callout}

## Definition

The **universal set** $\Omega$ is the set that contains all objects that could conceivably be of interest in a particular context.

By definition the, any set $S$ must be a subset of $\Omega$.

:::

## More definitions

The universal set is important because it defines the scope of our analysis. Say we are studying the performance of students of Statistics I in `r year(Sys.Date())`. 

The cars parked outside our institution do not belong to the universal set, because they are not relevant for our purpose. Only students of Statistics I in `r year(Sys.Date())` belong to the universal set.

## Set Operations

:::{.callout}
## Definition
The **complement** of a set $S$, with respect to $\Omega$, is the set $\{x\in\Omega| x\notin S\}$, that is, all the relevant elements that do not belong in $S$. We denote it as $S^c$.

> **Corollary:** It is easy to see that $\Omega^c=\emptyset$.
:::

## Set Operations

:::{.callout}

## Definition

The **union** of two sets $S$ and $T$ is the set of all elements that belong to $S$ *or* $T$ (or both), and is denoted by $S\cup T$. $$S\cup T=\{x\in\Omega | x\in S\ \vee\ x\in T\}$$

:::

:::{.callout}

## Definition

The **intersection** of two sets $S$ and $T$ is the set of all elements that belong to $S$ *and* $T$, and is denoted by $S\cap T$. $$S\cap T=\{x\in\Omega | x\in S\wedge x\in T\}$$

:::

Note that $\vee$ stands for *or*, and $\wedge$ stands for *and*.

## Set Operations

Sometimes we might need to consider the union or intersection of many sets, and for that we can use a notation simmilar to the one we used for summations:

* $$\bigcup_{n=1}^\infty S_n = S_1 \cup S_2 \cup ... = \{x\in\Omega | x \in S_n \text{ for some } n\}$$

* $$\bigcap_{n=1}^\infty S_n = S_1 \cap S_2 \cap ... = \{x\in\Omega | x \in S_n \text{ for every } n\}$$

## Set Operations

:::{.callout}
## Definition

Two sets (say $S$ and $T$) are said to be **disjoint** if $S\cap T=\emptyset$.

More generally, a collection of sets $S_n$ is disjoint if $S_i$ and $S_j$ are disjoint when $i\neq j$.

:::

. . .

:::{.callout}
## Definition

A collection of sets is said to be a **partition** of a set $S$ if the sets in the collection are:

* Disjoint

* Their union is $S$

:::

## Set Definition

:::{.callout}

The number of elements of a set $S$ is known as its **cardinality** and it is denoted as $\# S$. $\# S$ satisfies:

1. $\# S \geq 0$
2. $\# \emptyset = 0$

:::

## Set definition

:::{.callout}

If we have two sets, $S$ and $T$, we define the operation *set minus* as $\setminus$  the set that contains all elements of $S$ that do not belong to $T$

$$S\setminus T = \{x\in S| s\notin T\}$$

:::

## Back to Probability

In probability, $\Omega$, the universal set, is a non-empty set that contains all possible **outcomes** of an experiment. Each outcome is represented by $\omega$, and obviously $\omega\in\Omega$.

## Back to Probability

The sample space ($\Omega$) can be:

* Discrete, when $\#\Omega$ is finite, or countable infinite.
* Continuous, when $\#\Omega$ is uncountable.

## Back to Probability

Consider the experiment of throwing a die :game_die: and noting the number shown on side facing upwards.

1. The sample space is $\Omega=\{1,2,3,4,5,6\}$
2. In this case $\# \Omega = 6$
3. $\Omega$ is discrete.

## Back to Probability

Consider now the random experiment of measuring the life expectancy of a lamp :bulb:, measured in hours.

1. The sample space is $\Omega=\{x\in\mathbb{R}|x\geq 0\}$
2. In this case, $\Omega$ is all non-negative real numbers.
3. $\Omega$ is continuous.

. . .

Remember, $\Omega$ must include all possible outcomes from your experiment! Even then ones that seem ludicrous.

## Back to Probability

::: {.callout}

## Definition

A subset $A$ of the sample space $\Omega$ is called an **event**. $$A\subseteq \Omega$$

By definition then $\Omega$ is also an event.

:::

::: {.callout}

## Definition

We call the **realization** of an event $A$ if, after an experiment, outcome $\omega$ is realized, and $\omega \in A$.

:::

## Example

Let's go back to our experiment with the :game_die:

The sample space is: $\Omega=\{1,2,3,4,5,6\}$

Within this space, we can define the following events:

1. $A=\{1,3,5\}$, *i.e.* the number is odd.
2. $B=\{3,4,5,6\}$, *i.e.* the number is at least 3.
3. $C=\{1,2,3\}$ , *i.e.* the number is lower than 4.
4. $D=\{6\}$, *i.e.* the number is larger than 5.

## Example

Now let's revisit the example of our :bulb:

The sample space is: $\Omega=\{x\in\mathbb{R}|x\geq 0\}$

In this space, we can define the following events:

1. $A=\{x\in\mathbb{R}|75<x<95\}$, *i.e.* the :bulb: lasts between 75 and 95 hours.
2. $B=\{x\in\mathbb{R}|x\leq 100\}$, *i.e.* the :bulb: lasts no longer than 100 hours.
3. $C=\{x\in\mathbb{R}|x\geq 60\}$, *i.e.* the :bulb: lasts at least 60 hours.

## Events

::: {.callout}

## Definitions

* An **elementary event** is any event that contains a single element (*i.e.* $\# A = 1$)

* An **impossible event** is an event with no outcome, (*i.e.* $\# A=0$). As a consequence, an impossible event coincides with the empty set $\emptyset$.

* A **certain event** is indeed the event $\Omega$, as for any outcome we obtain $\omega$, this outcome belongs to the sample space $\Omega$ by definition.

:::

## Mixing up Sets and Probability

Consider two events $A$ and $B$ both subsets of $\Omega$

1. $A^c$ contains all the outcomes that are not in $A$. $A^c$ is the event of *not $A$*.

2. If  $A\subseteq B$, then an outcome that realizes event $A$ ($\omega\in A$), also realizes $B$, as $A\subseteq B\Rightarrow \omega\in B$ as well. $A\Rightarrow B$

3. For $A\cup B$ to happen, we need $\omega \in A$ **or** $\omega \in B$, which means that $A$ happens, or $B$ happens, or both happen simultaneously.

## Mixing up Sets and Probability

4. For $A\cap B$ to happen, we need $\omega \in A$ **and** $\omega \in B$, which means that $A$ and $B$ happen simultaneously.

5. $A$ and $B$ are incompatible if $A\cap B=\emptyset$, *i.e.* if an outcome is in one set, it cannot be in another, for example it cannot be that $A$ and $A^c$ happen simultaneously!

## Inherited set properties for events

Consider two events $A$ and $B$ both subsets of $\Omega$

1. Commutativity: $A\cup B = B\cup A$; $A\cap B=B\cap A$
2. Associativity: $(A\cup B)\cup C=A\cup(B\cup C)$; $(A\cap B)\cap C=A\cap(B\cap C)$
3. Distributivity: $A\cup(B\cap C)=(A\cup B)\cap(A\cup C)$; $A\cap (B\cup C)=(A\cap B)\cup(A\cap C)$
4. Morgan Law's: $(A\cap B)^c=A^c\cup B^c$; $(A\cup B)^c=A^c\cap B^c$

## Inherited set properties for events

5. $\left(A^c\right)^c=A$
6. Complement law: $A\cup A^c=\Omega$; $A\cap A^c=\emptyset$
7. Identity element: $A\cup\emptyset = A$; $A\cap\Omega = A$
8. Absorbing element: $A\cup\Omega = \Omega$; $A\cap\emptyset = \emptyset$
9. Idempotent law: $A\cup A=A$ ; $A\cap A=A$
10. $A\subset B\Rightarrow A\cap B=A$; $A\subset B\Rightarrow A\cup B = B$

## Example

Consider the sample space $\Omega =\{1,2,3,4,5,6\}$, from our :game_die: case.

Define the events: $$A=\{1\},\ B=\{3,6\},\ C=\{2,4,6\},\ D=\{4,5,6\}$$

## Example

Let's define the following events defined in $\Omega$

* $A\cup B$
* $A\cap B$
* $A^c$
* $(A\cup B)^c$
* $(B\cap C)^c$
* $B\setminus C$
* $C\setminus D$

## Probability

Kolmogorov's defined a set of characteristics that any probability $P$ measure should have, these are called the Kolmogorov's axioms (1933):

1. $P(A)\in\mathbb{R}$ and $P(A)\geq 0$, for any event $A\subseteq\Omega$.
2. $P(\Omega)=1$
3. For any $A$ and $B$ disjoint, $P(A)+P(B)=P(A\cup B)$

## Concept of probability

Besides the concepts we already saw of frequency and subjectivity for probability, there was an older, called *"classic"* one. This one was introduced by Pierre-Simon **Laplace** in 1812.

::: {.callout}

## Laplace or Classic interpretation of probability

Let $A$ be an event defined over a finite $\Omega$. The probability of event $A$ is defined as:

$$P(A)=\frac{\# A}{\# \Omega}$$

:::

## Example

Consider now an experiment throwing two dice :game_die: :game_die:

1. How many outcomes are in $\Omega$? $6^2=36$, $\#\Omega=36$.
2. Let $A$ be the event where both dice show the same number: $$A=\{(1,1), (2,2),...,(6,6)\}$$ here $\# A=6$
3. The probability that both dice show the same number is $$P(A)=\frac{\# A}{\# \Omega}=\frac{6}{36}=\frac{1}{6}$$

## Laplace or Classic interpretation of probability

The problem with this interpretation, is that we cannot use it, or it becomes meaningless, it when $\Omega$ is uncountable or infinite. Also, what if the outcomes are not equally likely? (*i.e.* if the dice are not fair?)

## Frequency interpretation

This is today still the dominant interpretation of probability.

In this case, what we want is to observe several independent repetitions of the experiment. After a while, some statistical regularity begins to emerge.

## Frequency interpretation

Logically, if you run an experiment, and are interested in the probability of event $A$, then the events you are registering are $A$ and $A^c$ or *not $A$*.

Every time you run your experiment, you count when you get an $A$ and when you observe an $A^c$ event. Obviously, the total number of experiments is how many times you observed $A$ and how many times you observed $A^c$.

## Example:

Experiment: Draw a random number in the interval $[0,1]$. $A$ denotes $x<0.4$.

```{r}
#| echo: false

# Define the values of N
N_values <- c(1, 10, 50, 100, 500, 2000, 5000, 10000, 50000)

# Initialize vectors to store results
event_A <- numeric(length(N_values))
not_event_A <- numeric(length(N_values))
total <- numeric(length(N_values))

# Simulate experiments
set.seed(42)
for (i in seq_along(N_values)) {
  trials <- runif(N_values[i])  # generate random numbers between 0 and 1
  event_A[i] <- sum(trials < 0.4)
  not_event_A[i] <- sum(trials >= 0.4)
  total[i] <- N_values[i]
}

# Create the data frame
results <- data.frame(
  Event_A = event_A,
  Not_Event_A = not_event_A,
  Total_Experiments = total,
  P_A = event_A/total
)
```


<table>
  <tr>
    <th>$A$</th>
    <th>$A^c$</th>
    <th>$N$</th>
    <th class="fragment" data-fragment-index="6">$P(A)$</th>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="0">`r results[1,1]`</td>
    <td class="fragment" data-fragment-index="0">`r results[1,2]`</td>
    <td class="fragment" data-fragment-index="0">`r results[1,3]`</td>
    <td class="fragment" data-fragment-index="7">`r results[1,4]`</td>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="1">`r results[2,1]`</td>
    <td class="fragment" data-fragment-index="1">`r results[2,2]`</td>
    <td class="fragment" data-fragment-index="1">`r results[2,3]`</td>
    <td class="fragment" data-fragment-index="7">`r results[2,4]`</td>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="2">`r results[3,1]`</td>
    <td class="fragment" data-fragment-index="2">`r results[3,2]`</td>
    <td class="fragment" data-fragment-index="2">`r results[3,3]`</td>
    <td class="fragment" data-fragment-index="7">`r results[3,4]`</td>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="3">`r results[4,1]`</td>
    <td class="fragment" data-fragment-index="3">`r results[4,2]`</td>
    <td class="fragment" data-fragment-index="3">`r results[4,3]`</td>
    <td class="fragment" data-fragment-index="7">`r results[4,4]`</td>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="4">`r results[5,1]`</td>
    <td class="fragment" data-fragment-index="4">`r results[5,2]`</td>
    <td class="fragment" data-fragment-index="4">`r results[5,3]`</td>
    <td class="fragment" data-fragment-index="7">`r results[5,4]`</td>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="5">`r results[6,1]`</td>
    <td class="fragment" data-fragment-index="5">`r results[6,2]`</td>
    <td class="fragment" data-fragment-index="5">`r results[6,3]`</td>
    <td class="fragment" data-fragment-index="7">`r results[6,4]`</td>
</table>

## Example:

```{r}
library(ggplot2)

# Define the values of N
N_values <- seq(1, 5000, by = 50)

# Initialize vectors to store results
event_A <- numeric(length(N_values))
not_event_A <- numeric(length(N_values))
total <- numeric(length(N_values))

# Simulate experiments
set.seed(42)
for (i in seq_along(N_values)) {
  trials <- runif(N_values[i])  # generate random numbers between 0 and 1
  event_A[i] <- sum(trials < 0.4)
  not_event_A[i] <- sum(trials >= 0.4)
  total[i] <- N_values[i]
}

# Create the data frame
results <- data.frame(
  Event_A = event_A,
  Not_Event_A = not_event_A,
  Total_Experiments = total,
  P_A = event_A/total
)

ggplot(data = results, aes(x = total, y = P_A)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Experiments", y = expression(P(A)))

```

## Frequency interpretation

As you can see, the more experiments we run, the more stabilized the ratio of occurrences for $A$ over the total number of experiments. More generally:

$$P(A)=\lim_{N\rightarrow \infty}\frac{A\text{ occurrences}}{N \text{- Number of Experiments}}$$

This is the relative **f**requency of $A$ in $N$ experiments: $f_A$

> Not always possible to repeat that many times the experiment in the **same** conditions.

## About our previous example

 It seems that the probability that the random number between 0 and 1 is below 0.4 is approximately 40%. The more experiments we run, the closer our relative frequency is to that number.

$$P(A)\underset{N \rightarrow \infty}{\rightarrow} 0.4$$

By the way, we will see later that theoretically, indeed $P(A)=0.4$